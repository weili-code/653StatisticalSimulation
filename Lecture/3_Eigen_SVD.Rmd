---
title: "MAT 653: Statistical Simulation"
author: Instructor$:$ Dr. Wei Li
date: "`r Sys.Date()`"
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  html_document:
    df_print: paged
    mathjax_config:
      - TeX-AMS-MML_HTMLorMML   
  pdf_document: 
      include:
        in_header: preamble_rmd.tex
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


<span style="background-color: #f5f5f5; padding: 5px; display: block;">
    <span style="width: 25%; display: inline-block; text-align: left;">
        [Back](javascript:window.history.back())
    </span>
    <span style="width: 30%; display: inline-block; text-align: center; color: grey;">
        **Updated:** `r Sys.Date()`
    </span>
    <span style="width: 35%; display: inline-block; text-align: right; color: grey;">
        Statistical Simulation, Wei Li
    </span>
</span>

<!------------->


# Eigenvalue decomposition

Given a square matrix $M$ so that $Mx=\lambda x$ for some nonzero vector $x$ and some $\lambda$(scalar). We call $\lambda$ an eigenvalue of $M$ whose associated eigenvector is $x$.

Note: $A$ is a square matrix, $\det(A)=\prod_{i=1}^n \lambda_i$. A square matrix is invertible if and only if its eigenvalues are non-zero.

**Eigen-decomposition:** $A$ is a square and symmetric matrix, then we can write $A=P \Lambda P^T$ where 
\begin{align*}
&P=[u_1, u_2,\cdots, u_n], \quad u\mbox{'s are the orthonormal eigenvector of }A \\
&\Lambda = diag\{\text{eigenvalues of }A\}.
\end{align*}

Note: If this symmetric matrix $A$ is invertible, then we have $A^{-1}=P\Lambda^{-1}P^T$ where 
\begin{equation*}
\Lambda^{-1} = 
\begin{pmatrix}
\ddots & 0 & 0 \\
0 & \frac{1}{\lambda_i} & 0 \\
0 & 0 & \ddots.
\end{pmatrix}
\end{equation*}

\begin{mdframed}
Example: $A=\begin{pmatrix}
 9 & 0 \\
0 & 4 
\end{pmatrix}$, $Av = \lambda v\implies (A-\lambda I)v=0$. Here we need a non-zero solution, so
$$
\det(A-\lambda I)=\left|
\begin{matrix} 
   9-\lambda & 0   \\
   0 & 4-\lambda
\end{matrix}
   \right| =0 \implies \lambda_1=9,\lambda_2=4
$$

For $\lambda_1=9$, since we know  $Av_1=9v_1\implies (A-9I)v_1=0$, then we have
$$
\begin{pmatrix} 
   0 & 0   \\
   0 & -5
\end{pmatrix} v_1 =
\begin{pmatrix} 
   0    \\
   0 
\end{pmatrix}
\implies v_1=\begin{pmatrix} 
   1   \\
   0
\end{pmatrix}
$$
For $\lambda_2=4$, since we know  $Av_2=4v_2\implies (A-4I)v_2=0$, then we have
$$
\begin{pmatrix} 
   5 & 0   \\
   0 & 0
\end{pmatrix} v_2 =
\begin{pmatrix} 
   0    \\
   0 
\end{pmatrix}
\implies v_2=\begin{pmatrix} 
   0   \\
   1 
\end{pmatrix}
$$
Now $A=P\Lambda P^T=
\begin{pmatrix} 
   1 & 0   \\
   0 & 1
\end{pmatrix}
\begin{pmatrix} 
   9 & 0   \\
   0 & 4
\end{pmatrix}
\begin{pmatrix} 
   1 & 0   \\
   0 & 1
\end{pmatrix}$ where $P=[v_1,v_2]$.
\end{mdframed}



**Singular value decomposition**
===========

For a $m \times n$ matrix ${A}$, $A^{T}A$ is a symmetric matrix. Let $\{ v_{1},\ldots,v_{n} \}$ be the collection of all the orthonormal eigenvectors of $A^{T}A$ (eigen-decomposition); let  $\lambda_{1},...,\lambda_{n}$ be the associated eigenvalues, $\lambda_{1} \ge \lambda_{2} \ge \cdots \geq \lambda_{n} \ge 0$. We have $\lVert Av_{i}\lVert^{2} = \lambda_{i}$, since 
$$
A^{T}Av_{i} = \lambda_{i}v_{i} \Rightarrow
v_{i}^{T}A^{T}Av_{i} = \lambda_{i}v_{i}^{T}v_{i}\lambda_{i}.
$$
The singular values of $A$ are squared roots of the eigenvalues of $A^{T}A$, denoted by $\sigma_{i} = \sqrt{\lambda_{i}}$; $\sigma_{1} \ge \sigma_{2} \ge \cdots \geq \sigma_{n} \ge 0$

**Fact**: The rank of a matrix $A$ is equal to the number of positive singular value of $A$.  

**SVD**: Let $A$ ($m \times n$) be a matrix of rank $r$. There exists a matrix $\Sigma_{m \times n}$ of the following form, with $D$ being a diagonal matrix whose entries are the first $r$ (non-zero) singular value. That is  
$$ \Sigma_{m \times n} = 
\begin{bmatrix}
  D & 0 \\
  0 & 0
\end{bmatrix},
$$
$$
 D = \begin{bmatrix}
      \sigma_{1} \\ & \ddots & \\ & & \sigma_{n}
     \end{bmatrix}.
$$
The Singular Value Decomposition of A is:
$$
  A = U_{m \times m}\Sigma_{m \times n}V^{T}_{n \times n}
$$
Where $U$ and $V$ are both orthogonal matrices: 
$V=[v_1, \cdots, v_n]$ consists of all orthonormal eigenvectors of $A^{T}A$; 
$U=[u_1, \cdots, u_m]$ consists the column $u_{i}$ as follows
$$\text{for }   1 \le i \le r, \quad u_{i} = \frac{Av_{i}}{\lVert Av_{i}\rVert} = \frac{Av_{i}}{\sigma_{i}},
$$
and these columns $\{ u_{1}, \ldots, u_{r}\}$ can be extended to $\{ u_{1}, \ldots, u_{m}\}$ as the orthonormal basis. 

Matrix $U$ and $V$ are not uniquely determined in general, but have the property: $Col(U)$ spans $Col(A)$ and $Col(V)$ spans $Row(A)$.


Reduced (thin) SVD 
-----------------------

For those $A_{m \times n}$, $U_{m \times m}$, $V_{n \times n}$, $\Sigma_{m \times n}$ above, we have the partition:  
$$
  U = [U_{r},U_{m-r}],  \\
  V = [V_{r},V_{n-r}],  \\
  \Sigma = \begin{bmatrix}
  D & 0 \\
  0 & 0
\end{bmatrix}
$$
Consequently, the SVD of $A$ can be represented as:  
$$
  A =
  \begin{bmatrix}
    U_{r} & U_{m-r}
  \end{bmatrix}
  \begin{bmatrix}
    D & 0 \\
    0 & 0
  \end{bmatrix}
  \begin{bmatrix}
    V_{r}^{T} \\
    V_{n-r}^{T}
  \end{bmatrix}
  = U_{r}DV_{r}^{T}
$$

Application
---------------------------

1. Linear Least Squares  
---------------------------

We can use thin SVD to solve the Least Squares Problems as following:  

\begin{align*}
 Ax &= b, \\
 A^{T}Ax &= A^{T}b, \\
 (U_rDV_r^{T})^{T}(U_rDV_r^{T})x &= (U_rDV_r^{T})^{T}b, \\
  V_rDIDV_r^Tx &= V_rDU_r^{T}b, \\
  DV_r^{T}x &= U_r^{T}b.
\end{align*}

Denote $w = V_r^{T}x$, $y = U_r^{T}b$, we have the following algorithm:  

**Algorithm**  

1. Find the SVD of $A = U_rDV_r^{T}$;  
2. Compute $y = U_r^{T}b$;  
3. Solve the diagonal system $Dw=y$, giving $w^{*} = D^{-1}y$;  
4. Solve $V_r^{T}x = w^{*} \Rightarrow x^* = V_rw^{*}$;  

The solution of the equation is $x^* = V_rD^{-1}U_r^{T}b= V\Sigma^{-1}U^{T}b$. 
We notice that this SVD method allows $A$, $b$ to be arbitrary. Notice that $V_{r}D^{-1}U_{r}^{T}$ is like the "inverse" of A. Here we have the concept of generalized inverse.  

**Moore-Penrose inverse of A**: $A^{\dagger} = V_{r}D^{-1}U_{r}^{T}$

2.LS-Problem 
-----------------------

In LS Problem 
$$\min_{x} \lVert Ax -b \rVert,$$ 
where $A$ and $b$ are not restricted at all, we have:  
$$
  A^{T}Ax = A^{T}b.
$$
Let $\mathfrak{L}$ is the set of all the minimizers to the LS problem. We have following facts.  

**Fact 1** $x^{*} = A^{\dagger}b \in \mathfrak{L}$; from the normal equation, $A^{T}AA^{\dagger} = A^{T}.$  

**Fact 2** $A\tilde{x_{1}} = A\tilde{x_{2}}$, for any $\tilde{x_{1}},\tilde{x_{2}} \in \mathfrak{L}$.  

**Fact 3** For the optimization problem $\min_{x \in \mathfrak{L}} \lVert x\rVert$, there is a unique solution $x^{*} = A^{\dagger}b$.  

**Fact 4** We already have the result that if we choose some $\lambda>0$, LS problem will has a unique solution to the "modified" normal equation:
$$
 (A^{T}A+\lambda I)x=A^{T}b,
$$
that is, $\hat{x} = (A^{T}A+\lambda I)^{-1}A^{T}b.$ In fact, let $\lambda \rightarrow 0$, we have $(A^{T}A+\lambda I)^{-1}A^{T} \rightarrow A^{\dagger}.$ So 
$$
 x^*= \lim_{\lambda \to 0} [(A^{T}A+\lambda I)^{-1}A^{T}]b= \lim_{\lambda \to 0} [A^T(AA^T+\lambda I)^{-1}]b = A^{\dagger} b.
$$

**Fact 5** The projection of $b$ on $Col(A)$ is given by $A(A^{T}A)^{-1}A^{T}b$ (assuming $A^{T}A$ is invertible), here is a more general result  
$$
  \hat{b} = AA^{\dagger}b = \lim_{\lambda \rightarrow 0}[A(A^{T}A+\lambda I)^{-1}A^{T}]b,
$$
so $P_A:=AA^{\dagger}$ is orthogonal projection onto $Col(A)$, $I-P_A$ is orthogonal projection onto $\op{Ker}(A^T).$




<!------------->
<span style="background-color: #f5f5f5; padding: 5px; display: block;">
    <span style="width: 25%; display: inline-block; text-align: left;">
        [Back](javascript:window.history.back())
    </span>
    <span style="width: 30%; display: inline-block; text-align: center; color: grey;">
        **Updated:** `r Sys.Date()`
    </span>
    <span style="width: 35%; display: inline-block; text-align: right; color: grey;">
        Statistical Simulation, Wei Li
    </span>
</span>


