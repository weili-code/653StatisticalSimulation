---
title: "Basics of Markov Chains"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Jingyao Tang   
date: Nov 2nd, 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  pdf_document: 
      latex_engine: xelatex
      include:
        in_header: preamble_rmd.tex
  html_document:
    df_print: paged
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

# Discrete Markov Chain
$\{ X^{(n)}: n=0, \ldots,  \}$ a sequence of dependent random variables, each $X^{(n)} \in S$ (state space). 

We first discuss M.C in a discrete state space, say, $S = \{1,2, \cdots , k\}$.

**Definition:** A Markov chain is a sequence of random variables $X^{(n)}$ such that
\begin{align*} 
&P(X^{(n)}\in A_n | X^{(n-1)}\in A_{n-1},\cdots, X^{(0)}\in A_0)\\
&= P(X^{(n)}\in A_n | X^{(n-1)}\in A_{n-1})
\end{align*}

Let 
$$p^{[ n-1,n ]}_{i,j} = P(X^{(n)}=j| X^{(n-1)}=i) \qquad \text{Transition Kernel}$$ 
We'll restrict ourselves to the time-invariant M.C.,that is, $p^{[n-1, n]}_{i,j}$ does not depend on the time reference point, but only depend on how far apart the two time points are. So $p^{[n, n+1]}_{i,j}=p^{[n-1, n]}_{i,j}=\cdots=p^{[0, 1]}_{i,j}$ holds etc.

**one-step transition kernel**

- transition probability
$$p_{i,j}=p^{[1]}_{i,j}: = P(X^{(n+1)}=j| X^{(n)}=i)$$

- Transition matrix
$$P := \begin{bmatrix}
p_{11} & p_{12} & \cdots & p_{1k}\\
p_{21} & p_{22} & \cdots & p_{2k}\\
 \vdots & \ddots & \cdots & \vdots\\
p_{k1} & p_{k2} & \cdots & p_{kk}\\
\end{bmatrix}$$

$$p_{i,j} \geq 0, \  \forall i=1,\cdots, k, \  \sum_{j=1}^kp_{i,j} = 1$$

**m-step transition kernel**

- transition probability
$$p^{[m]}_{i,j} := P(X^{(n+m)}=j| X^{(n)}=i)$$

- Transition matrix
$$P^{[m]} := \begin{bmatrix}
p_{11}^{[m]} & p_{12}^{[m]} & \cdots & p_{1k}^{[m]}\\
p_{21}^{[m]} & p_{22}^{[m]} & \cdots & p_{2k}^{[m]}\\
 \vdots & \ddots & \cdots & \vdots\\
p_{k1}^{[m]} & p_{k2}^{[m]} & \cdots & p_{kk}^{[m]}\\
\end{bmatrix}$$

## State distribution (occupation prob dist)

**Time 0**
$$\alpha^{(0)} = (\alpha_1^{(0)}, \cdots,\alpha_k^{(0)}), \text{where} \ P(X^{(0)}=i)=\alpha_i^{(0)}$$

**Time 1**
$$\alpha^{(1)} = (\alpha_1^{(1)}, \cdots,\alpha_k^{(1)})= \alpha^{(0)}P$$
One can show that $\alpha^{(m)}=\alpha^{(0)}P^m$.

\begin{mdframed}
**Proof**
\begin{align*}
\alpha_j^{(1)} &= P(X^{(1)} = j)\\
&= \sum_{i=1}^kP(X^{(1)}=j, X^{(0)}=i)\\
&= \sum_{i=1}^kP(X^{(1)}=j| X^{(0)}=i)P(X^{(0)}=i)\\
&= \sum_{i=1}^k \alpha_i^{(0)}p_{i,j}\\
&= \alpha^{(0)}P\\
\alpha^{(2)} &=\alpha^{(1)}P=\alpha^{(0)}P^2\\
&\vdots\\
\alpha^{(m)} &=\alpha^{(0)}P^m
\end{align*}
\end{mdframed}

Also one can write $\alpha^{(m)}= \alpha^{(0)} P^{[m]}$.

Therefore, 
$$P^{[m]}=P^m$$

**Definition**
  If a state distribution($\pi$) satisfies:
$$\pi P = \pi$$
Then $\pi$ is called a "steady state dist" (stationary distribution).
note that it is the same as
\begin{align*}
\pi_j = \sum_{i=1}^k\pi_ip_{i,j}, \ \forall \ j = 1,2,\cdots , k
\end{align*}
The LHS $\pi_j$ is steady state prob for the state $j$. The RHS $\sum_{i=1}^k\pi_ip_{i,j}$ is total probability flowing into state $j$ from any states.
To see the meaning of the steady state distribution, consider a scenario that at some step n, we reached the steady state distribution,
\begin{align*}
\alpha^{(n)} &= \pi \\
\alpha^{(n+1)} &= \alpha^{(n)}P = \pi 
\end{align*}
Therefore, once we reached the steady state distribution, the state distribution becomes stationary. 

**Find the steady state distribution**:
For a given P, note that
$$P^T\pi^T = 1\pi^T$$
So $\pi^T$ is eigenvalue of $P^T$ that corresponds to eigenvalue 1.

# Markov Chain Monte Carlo methods(MCMC)
  Most of the M.C. encountered in MCMC settings enjoy some nice properties.
In particular, we're interested in M.C.s that satisfy the following three properties.

## aperiodic
- A state $i$ has period $v \in N$ if $p_{ii}^{[m]}>0$ only holds for $m$ that is a multiple of $v$ 

- A state is aperiodic if  $p_{ii}^{[m]}>0$ for all $m$ sufficiently large, that is $v = 1$

- A chain is called "aperiodic" if all states are aperiodic

## (positive) recurrent
- A state $i$ is said to be recurrent, if when we run the M.C. from $i$ continusously, we are guaranteed to return to i infinitely often and the first return happens within a finite numbers of steps on average 

- A chain is called recurrent if all state are.

## irreducible
  The chain has a positive probability of eventually reaching any state, i.e.
  $$\forall i,j \ p_{i,j}^{[n_0]} > 0 \ for \ some \ n_0$$

## Summary
  We restrict ourselves to "time inveriant" M.C. that are irreducible, aperiodic and (positive) recurrent. We call these M.C. "ergodic M.C.".

# Theorem
Any ergodic M.C. has a steay state distribution $\pi$, i.e., $\pi P = \pi$, and
$$\pi_j = \lim_{n \to \infty} p_{i,j}^{[n]}  \qquad \text{for all} \ i$$

that is, $\pi$ is also the "long-run" distribution of the chain.

\begin{mdframed}
Example: 
$$S = \{1,2,3\}$$
$$\alpha^{[0]} = {(1,0,0)}$$
$$P=\begin{bmatrix}
1/2 & 1/2 & 0\\
0& 1/2 & 1/2\\
1/5 & 0 & 4/5
\end{bmatrix}$$
\begin{align*}
P(X^{(1)}) & = \alpha^{[1]}\\
\alpha^{[1]} &=  \alpha^{[0]}P = (1/2 , 1/2 , 0)\\
P(X^{(2)}) &= \alpha^{[2]}\\
\alpha^{[2]} &=  \alpha^{[1]}P = (1/4 , 1/2 , 1/4)\\
&\ \vdots\\
\alpha^{[10]} &=  \alpha^{[0]}P^{10} = (2/9, 2/9 , 5/9)
\end{align*}
if n is large\\
$\alpha^{[n]}$ converges to (2/9, 2/9 , 5/9), is "long-run distribution".

Verify:
$(2/9, 2/9 , 5/9)P = (2/9, 2/9 , 5/9)$
\end{mdframed}

# Sampling from steady-state distribution
Let $\alpha^{(m)}$ be the state distribution at time m. How can we obtain a sample from the steady-state distribution? One possibility is that we start N chains, let them run long enough, and collect those near-end-time values from all these chains. These values then become a sample from the steady-state distribution. With ergodicity, there is another possibility (more feasible), that is we collect a few of near-end-time values from a single M.C. that has run long enough, then this collection also can be seen as a sample from the steady-state distribution.

- single chain
\begin{align*}
X^{(1)} &\sim \alpha^{(1)}\\
X^{(2)} &\sim \alpha^{(2)}\\
& \ \ \vdots\\
X^{(\infty)} &\sim \alpha^{(\infty)} \equiv \pi\\
\end{align*}

- ensemble: the set of all possible chains
$$\begin{bmatrix}
\text{chain 1} & \text{chain 2} &  \cdots & \text{chain N} \\
X^{(1)}_1 & X^{(1)}_2 & \cdots & X^{(1)}_N \\
X^{(2)}_1 & X^{(2)}_2 & \cdots & X^{(2)}_N \\
\vdots & \vdots &  \cdots &  \vdots \\
X^{(\infty)}_1 & X^{(\infty)}_2 & \cdots & X^{(\infty)}_N \\
\end{bmatrix}$$

# The Law of Large Number
A ergodic M.C. has a very important property. That is, the time average of a simple realization approaches the average of all possible realization of their chains, **called ensemble**, at some particular point in the far future.
  
**Goal:** estimate $E(h(X^{(\infty)})) = \int h(x)\pi(x)dx$

The conventional law of large number says that, using the ensemble, 
  $$\frac{1}{N}\sum_{i=1}^Nh(X^{(\infty)}_i) \rightarrow E(h(X^{(\infty)})) \text{ as } \ N \rightarrow \infty.$$

Ergodicity guarantees that, using just a single M.C., 
  $$\frac{1}{T}\sum_{n=1}^Th(X^{(n)}) \rightarrow E[h(X^{(\infty)})] \text{ as }\ T \rightarrow \infty.$$
  This provides the theoretical justification--why obtaining samples from a single M.C. after it has run long enough, we are effectively obtaining samples from the steady-state distribution.
 