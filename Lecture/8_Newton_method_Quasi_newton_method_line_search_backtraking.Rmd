---
title: "MAT 653: Statistical Simulation"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Jingyao Tang   
date: Sep 28th, 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  pdf_document: 
      latex_engine: xelatex
      include:
        in_header: preamble_rmd.tex
  html_document:
    df_print: paged
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


# Newton's Method
Take a second order Taylor expansion of $T(x_k)$ in the direction given by $\alpha p$:
$T(x_k,\alpha p)=f(x_k)+Î±p^T\nabla f(x_k)+1/2\alpha^2p^T\nabla^2f(x_k)p$.

we travel to a stationary point of this quadratic approximation which is a minimizer if f is convex. Set \ $\alpha \equiv 1$. 

Set gradient of $T(x_k,\alpha p)$ to zero and solve for p.

\begin{align*}
FOC: &\nabla_pT(x_k,\alpha p) \underset{set}{=} 0\\
&\nabla f(x_k) + \nabla^2f(x_k)p = 0\\
\Rightarrow  & p = p_k = -\nabla f(x_k)^{-1} \cdot \nabla f(x_k), \text{if} \ \nabla^2f(x_k) \ \text{is invertible}\\
&x_{k+1} \leftarrow x_k - \big[\nabla^2f(x_k)\big]^{-1} \cdot \nabla f(x_k)
\end{align*}

There are two ways to solve the system:

**1.** If $\nabla^2f(x_k)$ is not invertible, you can use $\big[\nabla^2f(x_k)\big]^+$.

**2.** $\nabla f(x_k) + \nabla^2 f(x_k)(x_{k+1} - x_k) = 0$.

$\Leftrightarrow \nabla^2f(x_k)x_{k+1} = \nabla^2f(x_k) x_k - \nabla f(x_k)$.

$\Rightarrow solve \ for \ x_{k+1}$

**Remark:**
Newton method produces a sequence of points $x_1 , x_2, ...$ that minimizes a sequence of the function by repeatedly creating a quadratic approximation to the function $f$.

With a quadratic approximation more closely mimicking the object function. Newton method is often more effective than the gradient method. However, the reliance on the quadratic approximation makes Newton's method more difficult to use, especially for non-convex function.

$\nabla^2f(x_k)$ may not be invertible, so can use
$\big[ \nabla^2f(x_k) \big]^+$ or solving for $x_{k+1}$ from this system as above.

However, both approaches do **NOT** guarantee that: 
$$p_k^T \nabla^2f(x_k) < 0$$
**Maximization Problem**

Note: $\max f(x) = \min (-f(x))$

**Newton method**
$$
x_{k+1} \leftarrow x_k -   \big[\nabla^2f(x_k)\big]^{-1} \cdot \nabla f(x_k)
$$
$$
\text{note:} \big[\nabla^2f(x_k)\big]^{-1} \text{is n.d. }
$$

**gradient descend method**
$$
x_{k+1} \leftarrow x_k - (-\nabla f(x_k))
$$

**Quasi Newton Method(minimization problem)**
\begin{align*}
  & p_k = -\big[\nabla^2f(x_k)\big]^{-1} \cdot \nabla f(x_k) \qquad \text{Newton Method}\\
  & p_k = -B_k^{-1} \nabla f(x_k) \qquad \text{Quasi -Newton  Method}
\end{align*}
here,  $B_k$ is some approximation to $\nabla^2f(x_k)$. Note that
\begin{align*}
  & \nabla f(x_{k+1}) = \nabla f(x_k) + \nabla^2f(x_k)(x_{k+1} - x_k) + o(||x_{k+1} - x_k||)\\
  & \Leftrightarrow \nabla^2f(x_k)(x_{k+1} - x_k) \small  \approx \nabla f(x_{k+1}) - \nabla f(x_k)
\end{align*}
I want my $B_k$ to satisfy:

1. $B_k$ is symmetric.
2. $B_k(x_{k+1} - x_k) = \nabla f(x_{k+1}) - \nabla f(x_k)$.

**BFGS**
\begin{align*}
B_{k+1} &= B_k - \frac{B_ks_ks_k^T B_k}{s_k^T B_ks_k} +  \frac{y_ky_k^T}{y_k^T s_k}\\ 
s_k &= B_k(x_{k+1} - x_k)\\
y_k &= \nabla f(x_{k+1}) - \nabla f(x_k)
\end{align*}

$\text{Whenever you have}: B_0 >0$ and $s_k^T y_k > 0 : (\text{Secant Condition})$, the BFGS update produces positive definite approximation to Hessian.

**Alternative version of BFGS** 

Update inverse of Hessians $B_k$:

Let $\tilde {B} = B_k^{-1}$.
\begin{align*}
\tilde{B}_{k+1}^{-1} &= (I-p_ks_ky_k^T)\tilde{B}(I-p_ky_ks_k^T) + p_ks_ks_k^T\\
p_k &= \frac{1}{y_k^T s_k}\\
p_k &= -\tilde{B} \nabla f(x_k)
\end{align*}

initial choice of $B_0:I,$ or some multiple of $I$.

# Line Search

choose $\alpha$ assume $p_k$ is found.
An ideal choice of $\alpha$ is find a global minimizer of\
$$  
  \phi(\alpha) \equiv f(x_k + \alpha p_k), \alpha > 0
$$
but too costly.

Instead, we use some inexact line search method to identify a step length that achieves adequate reduction in $f$.

## Sufficient reduction condition(Armijo condition)
\begin{align*}
f(x_k + \alpha p_k) &\leq f(x_k) + c_1\alpha \nabla f(x_k)^T p_k,
\end{align*}
here, $c_1$ can be some small number, say $10^{-4}$. Let $\phi(\alpha)= f(x_k + \alpha p_k)$, and $l(\alpha) = f(x_k)+c_1\alpha \nabla f^T(x_k) p_k$. The condition says we need to choose $\alpha$ such that there is sufficient reduction in $\phi(\alpha)$, i.e., $\phi(\alpha) \leq l(\alpha).$

## Procedure (Backtracking line search)
choose $\bar{\alpha} > 0, \rho \in(0,1), c_1 = 10^{-4}$.

In the k-th step of iteration, 
\begin{align*}
\text{ repeat until } &f(x_k + \alpha p_k) \leq f(x_k) + c_1\alpha p_k^T \nabla f(x_k)\\
&\alpha \leftarrow \rho \alpha \\
\text{ end(repeat) }&
\end{align*}
When terminated, $\alpha_k \leftarrow \alpha$.

## Algorithm (BFGS with line search)
$x_0, \epsilon>0, \tilde{B}_0, k \leftarrow 0$
\begin{align*}
&\text{ while } ||\nabla f(x_k)||>\epsilon:\\
& \qquad compute \ p_k = -\tilde{B}_k\nabla f(x_k)\\
& \qquad  line \ search \ for \ \alpha_k\\
& \qquad  x_{k+1}  \leftarrow x_k + \alpha_kp_k\\
& \qquad  s_k  \leftarrow x_{k+1} - x_k\\
& \qquad  y_k  \leftarrow \nabla f(x_{k+1}) - \nabla f(x_k)\\
& \qquad  compute \ \tilde{B}_{k+1}\\
& \qquad k \leftarrow k+1 \\
& \text{end while} 
\end{align*}


