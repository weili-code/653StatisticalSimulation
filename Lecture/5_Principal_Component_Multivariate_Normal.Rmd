---
title: "MAT 653: Statistical Simulation"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$   Waleed A. Raja
date: Sep 16th, 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  html_document:
    df_print: paged
  pdf_document: 
      include:
        in_header: preamble_rmd.tex
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


**Principal Component**

$X$: $n\times p$ data matrix assumed full column rank, $n$ is the number of cases, and $p$ is the number of features.
Apply reduced SVD:
$$
X=U\Sigma V^T
$$
$U$ is a ${n\times p}$ matrix, $\Sigma$ is a ${p\times p}$ matrix, and $V^T$ is a ${p\times p}$ matrix.\

Let $V_j$ be the $j$-th column of $V$, it is called $j^{th}$ **principal component direction** of $X$. Each element in $V_j$ are called **principal component loadings**. From SVD, $V_j^T X^T X V_j=\sigma_j^2$, thus $\|XV_j\|=\sigma_j$, where $\sigma_j$ are **singular values** of $X$.

The $j$-th **principal component** of $X$ is defined to be
$$
Z_j:=XV_j
$$
Specifically, $Z_j =(Z_{1j}, Z_{2j},\dots,Z_{nj})$. Elements in $Z_j$ are called **principal component scores**: 
$$Z_{ij} =X_{[i,.]}V_j=\frac{X_{[i,.]}V_j}{\langle{V_j},{V_j}\rangle} \text{},$$
which is the coefficient of projecting $X_{[i,.]}$ onto span of {$V_j$}.

The $j$-th **normalized principal component** of $X$ is defined to be
$$
U_j:=\frac{XV_j}{\|XV_j\|}=\frac{Z_j}{\sigma_j}.
$$

Let's assume columns of $X$ are centered about $0$, then we can see the sample variance of $Z_j=XV_j$ is 
$$\frac{\|XV_j\|^2}{n-1}=\frac{\sigma_j^2}{n-1}.$$

Since $\sigma_1^2\geq\sigma_2^2\geq\cdots\geq\sigma_n^2$, $Z_1=XV_1$ has the largest sample variance among all normalized linear combinations of columns of $X$; $Z_j$, $j\ge2$, has maximum sample variance subject to being orthogonal to the earlier ones. $Z_2\perp Z_1, Z_3\perp(Z_2, Z_1),\dots$ etc.

\begin{mdframed}
Check:
$$\begin{aligned}
Cov(Z_1,Z_2)&=\frac{Z_1^TZ_2}{n-1}\\
&=\frac{(XV_1)^TXV_2}{n-1}\\
&=\frac{V_1^TX^TXV_2}{n-1}\\
&=\frac{(V_1^TV)\sum\sum(V^TV_2)}{n-1}\\
&=0
\end{aligned}
$$
\end{mdframed}
Since $XV_j=Z_j$, $X[ V_1, \ldots, V_p]=[Z_1,Z_2,\dots,Z_p]$, i.e, $XV=Z$.


**Application**

One application for the principal components is the principal component regression: 

*Least Square Problem*\
$\underset{\beta}{\text{min}}\|X\beta-y\|^2$

*Principal Components Regression*\
$\underset{\theta}{\text{min}}\|Z\theta-y\|^2$

By construction, the first principal component will contain the most information about the data, the subsequent principal components contain less and less information about the data. Therefore, the first few principal components $Z_1,Z_2,\dots,Z_k$ $(k<p)$ can be used as predictors in lieu of the original set of all predictors in $X$.\



**Multivariate Normal**\

If $Z_1,Z_2,\dots,Z_p\overset{iid}\sim \mathcal{N}(0,1)$, $$Z=
\begin{bmatrix}
z_1\\
z_2\\
z_3
\end{bmatrix}$$
Then, $Z\sim \mathcal{N}(0,I)$\

Also, by the well known property of multivariate normal distribution, $\mu+AZ\sim\mathcal{N}(\mu,AA^T)$
$$\mu=
\begin{bmatrix}
\mu_1\\
\mu_2\\
\mu_3
\end{bmatrix}$$
$$AA^T=
\begin{bmatrix}
\sigma_1^2 & \sigma_{12}^2 & \cdots & \sigma_{1p}^2\\
\sigma_{21}^2 & \sigma_{2}^2 & \cdots & \sigma_{2p}^2\\
\vdots & \vdots & \ddots & \vdots\\
\sigma_{p1}^2 & \sigma_{p2}^2 & \cdots & \sigma_{p}^2
\end{bmatrix}$$
$AA^T$ is the variance/covariance matrix. It is a symmetric matrix.\

Goal: To generate W (multivariate), such that, 
$W\sim\mathcal{N}(\mu,\sum)$

Example: $p=2$
$$W=
\begin{bmatrix}
w_1\\
w_2
\end{bmatrix}$$
$$\mu=
\begin{bmatrix}
0.1\\
0.3
\end{bmatrix}$$
$$\sum=
\begin{bmatrix}
0.5 & 0.1\\
0.1 & 0.4
\end{bmatrix}$$
Take Cholesky decomposition of $\sum=U^TU$.
In R, $\mu+U^TZ \sim\mathcal{N}(\mu, U^TU)$