---
title: "MAT 653: Statistical Simulation"
author: Instructor$:$ Dr. Wei Li
date: "`r Sys.Date()`"
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  html_document:
    df_print: paged
    mathjax_config:
      - TeX-AMS-MML_HTMLorMML   
  pdf_document: 
      include:
        in_header: preamble_rmd.tex
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \usepackage{mathtools}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


<span style="background-color: #f5f5f5; padding: 5px; display: block;">
    <span style="width: 25%; display: inline-block; text-align: left;">
        [Back](javascript:window.history.back())
    </span>
    <span style="width: 30%; display: inline-block; text-align: center; color: grey;">
        **Updated:** `r Sys.Date()`
    </span>
    <span style="width: 35%; display: inline-block; text-align: right; color: grey;">
        Statistical Simulation, Wei Li
    </span>
</span>

<!------------->


# Metropolis-Hasting algorithm (blockwise)

**Target density:** $f(x,y)$, which could be unnormalized.

We use two candidate transition densities $q_{X} (x \vert x_c, y_c)$ and $q_{Y} (y \vert  x_c, y_c)$, where $x_c$, $y_c$ stand for current values.

**Algorithm:**

1. Update $X$   
   (a) sample $X^* \sim q_X(\cdot \vert  X^{(s)}, Y^{(s)})$   
   (b) compute $\gamma_X (X^* \vert  X^{(s)}, Y^{(s)})=\displaystyle \frac{f(X^{*}, Y^{(s)})}{f(X^{(s)}, Y^{(s)})} \cdot \frac{q_X(X^{(s)}\vert  X^{*}, Y^{(s)})}{q_X(X^{*} \vert  X^{(s)}, Y^{(s)})}$
   (c) set $X^{(s+1)}$ to $X^*$ with probability $\min (1, \gamma_X)$; set $X^{(s+1)}$ to $X^{(s)}$ otherwise.
   
2. Update $Y$   
   (a) sample $Y^* \sim q_Y(\cdot \vert  X^{(s+1)}, Y^{(s)})$   
   (b) compute $\gamma_Y (Y^* \vert  X^{(s+1)}, Y^{(s)})=\displaystyle \frac{f(X^{(s+1)}, Y^{*})}{f(X^{(s+1)}, Y^{(s)})} \cdot \frac{q_Y(Y^{(s)}\vert  X^{(s+1)}, Y^{*})}{q_Y(Y^{*} \vert  X^{(s+1)}, Y^{(s)})}$
   (c) set $Y^{(s+1)}$ to $Y^*$ with probability $\min (1, \gamma_Y)$; set $Y^{(s+1)}$ to $Y^{(s)}$ otherwise.
   
**Remark:** $\displaystyle p \big( (x,y) \to (x^*,y^*) \big) f(x,y) = p \big( (x^*,y^*) \to (x,y) \big) f(x^*,y^*)$ does not hold for this algorithm in general.   
   
\begin{mdframed}
Proof: 
\end{mdframed}



# Gibbs Sampling


Recall the block-wise Metropolis-Hastings: to sample from $f(X,Y)$, we used two candidates: $q_X(x|X,Y), q_Y(y|X,Y)$.

1. Update $X$:
\begin{align*}
\gamma_X(X^*|X^{(s)},Y^{(s)})&=\frac{f(X^*,Y^{(s)})}{f(X^{(s)},Y^{(s)})}\cdot\frac{q_X(X^{(s)}|X^{(s)},Y^{(s)})}{q_X(X^*|X^{(s)},Y^{(s)})}.
\end{align*}

Suppose we take $q_X(x|X^{(s)}, Y^{(s)})$ to be full condition distribution $f_{X|Y}(x|Y^{(s)})$, take $q_Y(y|X^{(s)},Y^{(s)})$ to be full condition distribution $f_{Y|X}(y|X^{(s)})$, then

\begin{align*}
\gamma_X(X^*|X^{(s)},Y^{(s)})&=\frac{f(X^*,Y^{(s)})}{f(X^{(s)},Y^{(s)})}\cdot\frac{q_X(X^{(s)}|X^{(s)},Y^{(s)})}{q_X(X^*|X^{(s)},Y^{(s)})}\\
&=\frac{f_{X|Y}(X^*|Y^{(s)})}{f_{X|Y}(X^{(s)}|Y^{(s)})}\cdot\frac{f_{X|Y}(X^{(s)}|Y^{(s)})}{f_{X|Y}(X^*|Y^{(s)})}\\
&=1
\end{align*}

2. Update $Y$. Similar argument goes through if we use the full conditional distributions as our candidate transition densities.

**Definition:**
Gibbs sampling is a special case of (blockwise) MH that take $q_X(x|X_c, Y_c)$ to be  $f_{X|Y}(x|Y_c)$, and take $q_Y(y|X_c,Y_c)$ to be  $f_{Y|X}(y|X_c)$, which then yield $\gamma_X=\gamma_Y=1$. So following this sampling scheme, all proposals are automatically accepted.


**Algorithm: Two stages Gibbs sampling**

\textbf{Target:} $f(X,Y)$, possibly unnormalized

Take $x^{(0)}$. For $s=1,2,\cdots$, generate

- $Y^{(s)}\sim f_{Y|X}(\cdot|X^{(s-1)})$
- $X^{(s)}\sim f_{X|Y}(\cdot|Y^{(s)})$

**Algorithm: Multi-stage Gibbs sampling**

\textbf{Target:} $f(X_1,X_2,\cdots,X_d)$, possibly unnormalized

Starting Values: $X^{(0)}=(X^{(0)}_1,X^{(0)}_2,\cdots,X^{(0)}_d)$. Let $f_{j}(X_j|X_{-j})$ denote the conditional density of $X_j$ given all the rest components $X_{-j}:=\{X_i: i\neq j\}$. 

The algorithm generates $X^{(s)}$ from $X^{(s-1)}$ as follows:

\begin{align*}
(1)&\quad X^{(s)}_1 \sim f_1(\cdot | X^{(s-1)}_2, \cdots,X^{(s-1)}_d)\\
(2)&\quad X^{(s)}_2 \sim f_2(\cdot | X^{(s)}_1, X^{(s-1)}_3, \cdots,X^{(s-1)}_d)\\
& \vdots \\
(d)&\quad X^{(s)}_d \sim f_d(\cdot | X^{(s)}_1, \cdots,X^{(s)}_{d-1})
\end{align*}

Repeat $s \leftarrow s+1$.

$\textbf{Advantage:}$ For high-dim problem, all the situation can be univariate and all probabilities are accepted.

**Remark**:

- The Gibbs sampler is a composition of MH moves with accept probability = 1. The move in each stage is reversible, but the composition itself is not.
- Both (Blockwise) MH and Gibbs sampling have the target distribution as the invariant distribution (steady-state distribution).

**Example: Two-stage Gibbs sampling**

\[f(x)\propto\frac{e^{-x^2/20}}{(1+(z_1-x)^2)(1+(z_2-x)^2)},\quad z_1=-4.3, z_2=5.2\]

Note that: $\displaystyle \frac{1}{1+(z_i-x)^2}=\int_0^\infty e^{-w_i(1+(z_i-x)^2)}\,dw_i$, then we can write
\[f(x,w_1,w_2)\propto e^{-x^2/20}\prod_{i=1}^2 e^{-w_i(1+(z_i-x)^2)}\]
so $f(x)$ is just the marginal pdf of $f(x,w_1,w_2)$.

Gibbs sampling: ${w}=(w_1, w_2)$

- $X^{(s)}\sim f_{X|W}(\cdot|{W}^{(s-1)})$
- $W^{(s)}\sim f_{W|X}(\cdot|X^{(s)})$


Here
\[f_{X|W}(x|w_1,w_2) \overset{x} \propto e^{-\big(\sum w_i\big)x^2+2x\sum w_iz_i}\cdot e^{-x^2/20}\sim N \Big(\frac{\sum w_iz_i}{\sum w_i+1/20},\frac{1}{2\big(\sum w_i+1/20\big)} \Big)\]
\[f_{W|X}(w_1,w_2|x)  \overset{W} \propto e^{-w_1(1+(z_1-x)^2)}\cdot e^{-w_2(1+(z_2-x)^2)}\]
Note from the factorization above, $w_1,w_2$ are independent given $x$, therefore,
\begin{align*}
f(W_1|X^{(s)}) & \overset{W_1} \propto \mbox{exponential}(1+(z_1-X^{(s)})^2)\\
 f(W_2|X^{(s)})  & \overset{W_2} \propto \mbox{exponential}(1+(z_2-X^{(s)})^2).
\end{align*}

**Multivariate Normal**

The multivariate normal distribution of a $d-$dimensional random vector $\mathbf{X}=(X_1, \cdots, X_d)^{\top}\in\mathbb{R}^d$ can be written in the notation $\mathbf{X} \sim \mathcal{N}(\boldsymbol\mu, \mathbf{\Sigma}).$ When the symmetric covariance matrix $\mathbf{\Sigma}$ is positive definite, then the multivariate normal distribution is non-degenerate, and the distribution has density function
\begin{align*}
f_{\mathbf{X}}(\mathbf{x} \vert  \boldsymbol\mu, \mathbf{\Sigma}) &= \frac{1}{(2 \pi)^{\frac{d}{2}}} \big( \det(\mathbf{\Sigma}) \big)^{-\frac{1}{2}} \exp \big(-\frac{1}{2} (\mathbf{x}-\boldsymbol\mu)^{\top} \mathbf{\Sigma}^{-1} (\mathbf{x}-\boldsymbol\mu)  \big)\\
&\overset{\mathbf{x}} \propto \exp(-\frac{1}{2} \mathbf{x}^{\top} \mathbf{\Sigma}^{-1} \mathbf{x} + \mathbf{x}^{\top} \mathbf{\Sigma}^{-1} \boldsymbol\mu)
\end{align*}
where $\mathbf{x}=(x_1, \cdots, x_d)^{\top}$.

For comparison, the density function of (univariate) normal distribution $X\sim N(\mu, \sigma^2)$
$$
f_X(x \vert  \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma} \exp \big(-\frac{1}{2\sigma^2}(x-\mu)^2 \big) \overset{x} \propto \exp\big(-\frac{1}{2\sigma^2}(x^2-2x\mu+\mu^2)\big) \overset{x} \propto \exp\big(-\frac{1}{2} x^2 (\sigma^2)^{-1}+ x(\sigma^2)^{-1}\mu\big).
$$


Bivariate normal 
$$\displaystyle (X,Y)\sim N \left(\begin{pmatrix}\mu_X\\ \mu_Y\end{pmatrix},\begin{pmatrix}\sigma_X^2 & \sigma_{XY}\\ \sigma_{XY}&\sigma_Y^2\end{pmatrix} \right),$$ 
Let $\rho=\frac{\sigma_{XY}}{\sigma_{X}\sigma_Y}$ be the correlation. It is a fact that
\[f_{X|Y}(x|y)\sim N(\mu_X+\rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y), \sigma_X^2(1-\rho^2))\]
\[f_{Y|X}(y|x)\sim N(\mu_Y+\rho\frac{\sigma_Y}{\sigma_X}(x-\mu_X), \sigma_Y^2(1-\rho^2)).\]

# Slice Sampling

The slice sampling is a technique based on the idea of the Fundamental Theorem of Simulation and Gibbs sampling.

- Sample from $f(x)$ some p.d.f supported on $\mathcal{X} \subset \mathbb{R}^d$.

Recall from the Fundamental Theorem of Simulation,
$$
f(x)=\int_{0}^{f(x)}1du.
$$
So $f(x)$ is a marginal density (in $X$) of the joint uniform distribution on the set
$$
\{(x, u): 0<u<f(x), x\in\mathcal{X},\}
$$
with the p.d.f. $f(x, u)=1_{\{(x, u): 0<u<f(x)\}}(x, u).$

To obtain a sample of $(X ,U)$ from this distribution, we can sample from a M.C. that has the target stationary distribution which can be done using Gibbs sampling, noting that
\begin{align*}
&f_{X|U}(x|u) \sim \op{unif}\{x: u< f(x) \} \\
&f_{U|X}(u|x) \sim \op{unif}\{u: 0< u< f(x) \}
\end{align*}

**Slice sampling**:

At iteration $s$, simulate

1. $U^{(s+1)} \sim \op{unif}(0, f(X^{(s)}))$
2. $X^{(s+1)} \sim \op{unif}(A^{(s+1)}),$ where $A^{(s+1)}:=\{x: f(x)>U^{(s+1)} \}.$


**Remark 1**: A major difficulty in the algorithm is to evaluate the set $A^{(s+1)}$. For a univaraite distribution, an iterative expansion is applied--called **stepping out**: star with some $L>0, R>0$ and consider the candidate interval $x \in (X^{(s)}-L, X^{(s)}+R)$, then we extend the interval until the endpoints fall outside the slice, that is, eventually we settle for some interval 
$$
\tilde{A}^{(s+1)}=\{x\in (X^{(s)}-L, X^{(s)}+R): f(x)>U^{(s+1)} \}
$$
Then we draw $X^*$ from $\tilde{A}^{(s+1)}$ using accept/reject method, i.e., we draw $X^*$ until $f(X^*)>U^{(s+1)}$ and set $X^{(s+1)} \leftarrow X^*$; otherwise, we adjust the interval by shrinking it, specifically, if $X^*>X^{(s)}$, set $R \leftarrow X^*-X^{(s)}$; if $X^*<X^{(s)}$, set $L \leftarrow X^{(s)}-X^*.$

**Remark 2**: The algorithm remains valid if we use $\tilde{f}$ the kernel of the p.d.f. in the algorithm.


**General form**:

Suppose $f(x) \propto \prod_{i=1}^k f_i(x)$, where $f_i(x)>0$ not necessarily density. Note that 
$f_i(x) = \int 1\{x: 0 \leq w_i \leq f_i(x) \}dw_i$. Thus $f$ is the marginal distribution of the joint distribution
$$
(x, w_1, \ldots, w_k) \overset{x, w_1, \ldots, w_k}{\propto} \prod_{i=1}^k 1\{w_i : 0\leq w_i \leq f_i(x) \},
$$
noting 
$$
f(x) \overset{x}{\propto} \int \cdots \int \prod_{i=1}^k 1\{w_i: 0 \leq w_i \leq f_i(x)\}dw_{1}\cdots dw_{k}= \prod_{i}f_i(x).
$$

**Slice sampling**:

At iteration $s$, simulate

1. $W_1^{(s+1)} \sim \op{unif}(0, f_1(X^{(s)}))$, 

   $W_2^{(s+1)} \sim \op{unif}(0, f_2(X^{(s)}))$, 
   
   $\vdots$
   
   $W_{k}^{(s+1)} \sim \op{unif}(0, f_k(X^{(s)}))$, 

2. $X^{(s+1)} \sim \op{unif}(A^{(s+1)})$, where $A^{(s+1)}:=\{x: f_i(x) \geq W_i^{(s+1)}, \forall i=1, \ldots, k\}.$


<!------------->
<span style="background-color: #f5f5f5; padding: 5px; display: block;">
    <span style="width: 25%; display: inline-block; text-align: left;">
        [Back](javascript:window.history.back())
    </span>
    <span style="width: 30%; display: inline-block; text-align: center; color: grey;">
        **Updated:** `r Sys.Date()`
    </span>
    <span style="width: 35%; display: inline-block; text-align: right; color: grey;">
        Statistical Simulation, Wei Li
    </span>
</span>
 