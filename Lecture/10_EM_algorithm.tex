% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  14pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{Your Name}
\fancyhead[C]{Institute Information}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{1pt}
\usepackage{amsmath}
\usepackage[linewidth=1pt]{mdframed}
\newcommand\inner[2]{\left\langle#1,#2\right\rangle}
\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}
\newcommand\mb[1]{\mathbf{#1}}
\newcommand\bs[1]{\boldsymbol{#1}}
\newcommand\mr[1]{\mathrm{#1}}
\newcommand\wh[1]{\widehat{#1}}
\newcommand\op[1]{\operatorname{#1}}
\newcommand\mbb[1]{\mathbb{#1}}
\usepackage{caption}
\usepackage{mathtools}
\captionsetup[figure]{labelformat=empty}
\newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={MAT 653: Statistical Simulation},
  pdfauthor={Instructor: Dr.~Wei Li},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{MAT 653: Statistical Simulation}
\author{Instructor\(:\) Dr.~Wei Li}
\date{2023-09-27}

\begin{document}
\maketitle

\setstretch{1.3}
{ { \href{javascript:window.history.back()}{Back} } { \textbf{Updated:}
2023-09-27 } { Statistical Simulation, Wei Li } }

\hypertarget{em-algorithm}{%
\subsection{EM Algorithm}\label{em-algorithm}}

Suppose we have \(n\) observables
\(x_{1},x_{2},...,x_{n} \stackrel{i.i.d}{\sim} g(x|\theta)\). Let
\(\boldsymbol{x}=\{x_i\}_{i=1}^n\). Our goal is to compute:
\begin{align*}
  \hat{\theta} = \arg\max L(\theta | \boldsymbol{x}) = \prod^{n}_{i=1} g(x_{i} | \theta)= f(\boldsymbol{x}|\theta),
\end{align*} where \(L(\theta | \boldsymbol{x})\) is the
\textbf{observed data likelihood function} and
\(f(\boldsymbol{x}|\theta)\) is the joint density.

Suppose there is some unobservables \(\boldsymbol{z}\) such that the
joint density of \((\boldsymbol{x} ,\boldsymbol{z})\) satisfies
\(f(\boldsymbol{x}|\theta)=\int f(\boldsymbol{x}, \boldsymbol{z}|\theta) d\boldsymbol{z}\).
Denote
\(L^{c}(\theta | \boldsymbol{x},\boldsymbol{z}) = f(\boldsymbol{x},\boldsymbol{z}|\theta)\),
which is the \textbf{complete data likelihood function}.

Since \begin{align*}
  (\boldsymbol{x},\boldsymbol{z}) \sim f(\boldsymbol{x},\boldsymbol{z}|\theta),
\end{align*} the conditional distribution of \(\boldsymbol{z}\) given
the observables \(\boldsymbol{x}\) is \begin{align*}
  f(\boldsymbol{z}|\boldsymbol{x},\theta) = \frac{f(\boldsymbol{x},\boldsymbol{z}|\theta)}{f(\boldsymbol{x}|\theta)}. 
\end{align*}

Use this result on the observed data likelihood function: \begin{align*}
  logL(\theta | \boldsymbol{x}) & = log f(\boldsymbol{x}|\theta) \\
                         & = log \frac{f(\boldsymbol{x},\boldsymbol{z}|\theta)}{f(\boldsymbol{z}|\boldsymbol{x},\theta)} \\
                         & = log(f(\boldsymbol{x},\boldsymbol{z}|\theta)) - log(f(\boldsymbol{z}| \boldsymbol{x},\theta)). 
\end{align*}

We shall use the notations:

\begin{align*}
E_{g}(f(\boldsymbol{x})) = \int f(\boldsymbol{x})g(\boldsymbol{x}) d\boldsymbol{x}, \qquad E_{\boldsymbol{z}|\boldsymbol{x}}(h(\boldsymbol{z},\boldsymbol{x})) = \int h(\boldsymbol{z},\boldsymbol{x})f(\boldsymbol{z}|\boldsymbol{x}) d\boldsymbol{z}. 
\end{align*}

Let \(\theta^{(0)}\) as our initial guess of the parameter, and take
conditional expectation of \(\boldsymbol{z}\) given \(\boldsymbol{x}\),
that is, the integral is taken with respect to
\(f(\boldsymbol{z}| \boldsymbol{x},\theta^{(0)})\), on both sides:

\begin{align*}
  E_{\theta^{(0)}}(log(L(\theta|\boldsymbol{x})))  = log(L(\theta|\boldsymbol{x})) & = E_{\theta^{(0)}}[logf(\boldsymbol{x},\boldsymbol{z}|\theta)|\boldsymbol{x}] - E_{\theta^{(0)}}[logf(\boldsymbol{z}|\boldsymbol{x},\theta)|\boldsymbol{x}] \\ 
  & = Q(\theta | \theta^{(0)},\boldsymbol{x}) - K(\theta | \theta^{(0)},\boldsymbol{x}), 
\end{align*} where we take \begin{align*}
Q(\theta | \theta^{(0)},\boldsymbol{x}) := E_{\theta^{(0)}}[logf(\boldsymbol{x},\boldsymbol{z}|\theta)|\boldsymbol{x}] \\
 K(\theta | \theta^{(0)},\boldsymbol{x}):= E_{\theta^{(0)}}[logf(\boldsymbol{z}|\boldsymbol{x},\theta)|\boldsymbol{x}] 
\end{align*}

It turns out for any candidate \(\theta^{'}\) for next iterate,
\begin{align*}
  K(\theta^{'}|\theta^{0},\boldsymbol{x}) \le K(\theta^{(0)} | \theta^{(0)},\boldsymbol{x}). 
\end{align*}

To see this,that is, for any \(\theta^{'}\): \begin{align*}
  E_{\theta^{(m)}}(\log f(\boldsymbol{z}|\boldsymbol{x},{\theta}^{'})|\boldsymbol{x}) \le E_{\theta^{((m))}}(\log f(\boldsymbol{z}|\boldsymbol{x},\theta^{(m)})|\boldsymbol{x})= \int \log f(\boldsymbol{z}| \boldsymbol{x},\theta^{(m)})f(\boldsymbol{z}|x,\theta^{(m)}) d\boldsymbol{z}. 
\end{align*}

Call
\(g(\boldsymbol{z}) = f(\boldsymbol{z}|\boldsymbol{x},\theta^{'})\),
\(h(\boldsymbol{z}) = f(\boldsymbol{z}|\boldsymbol{x},\theta^{(m)})\).
It suffices to show \begin{align*}
  E_{h}\Big[\log\frac{h(z)}{g(z)}\Big] \ge 0. 
\end{align*} The term \(E_{h}\Big[\log\frac{h(z)}{g(z)}\Big]\) is the
so-called Kullback-Leibler divergence \(KL(h||g)\) between the
distributions \(h\) and \(g\). To see the inequality, we use Jensen's
inequality:\\
\begin{align*}
  LHS & = \int \log \Big(\frac{h(z)}{g(z)}\Big) h(z) dz \\
      & = -\int \log \Big(\frac{g(z)}{h(z)}\Big) h(z) dz \\
      & \ge - log \int \frac{g(z)}{h(z)}h(z) dz = 0. 
\end{align*} One can also show that
\(E_{h}\Big[\log\frac{h(z)}{g(z)}\Big] =0\) if and only if \(h=g\).

So to maximize \(log(L(\theta|\boldsymbol{x})\) over \(\theta\), it
suffices to just maximize \(Q(\theta | \theta^{(0)},\boldsymbol{x})\)
over \(\theta\). By maximizing
\(Q(\theta | \theta^{(0)},\boldsymbol{x})\) over \(\theta\), one obtain
the maximizer \(\theta^{(1)}\) as the next iterate; we then by
maximizing \(Q(\theta | \theta^{(1)},\boldsymbol{x})\) over \(\theta\),
obtaining the next iterate \(\theta^{(2)}\)--the process can keep go on
until convergence.

\textbf{EM algorithm}

Based on the result, we have two main steps for EM algorithm: at the
\(m\)-th iteration,

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  E Step: compute \(Q(\theta |\theta^{(m)},\boldsymbol{x})\) as a
  function of \(\theta\) and \(\theta^{(m)}\).\\
\item
  M Step: find
  \(\theta^{(m+1)} = \arg\max\limits_{\theta} Q(\theta |\theta^{(m)},\boldsymbol{x})\).
\end{enumerate}

\textbf{Remark}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  EM algorithm only generates the limit point of \(\theta^{(m)}\) that
  is a stationary point of the objective function
  \(log(L(\theta|\boldsymbol{x})\). In practice, you'll try different
  starting values of \(\theta^{(0)}\).\\
\item
  If \(f(\boldsymbol{z}|\theta)\) does not depend on \(\theta\), then
  the algorithm is equivalent to that uses \[
  Q(\theta |\theta^{(m)},\boldsymbol{x})= E_{\theta^{(m)}}[logf(\boldsymbol{x}|\boldsymbol{z}, \theta)|\boldsymbol{x}].
  \]
\item
  Consider for \(h(x)=E[H(x,Z)]\) where the expectation is taken w.r.t
  to the random variable \(Z\). \begin{align*}
    \max\limits_{x} h(x) &= \max \limits_{x} E[H(x,Z)] \\
                     &= \max \limits_{x} E[H(X,Z)|X = x] \\ 
                     &= \max \limits_{x} \int H(x,z)f(z|x)dz, 
  \end{align*} we can use Monte Carlo to approximate the objective
  function: \begin{align*}
    \frac{1}{m}\sum^{m}_{i=1}H(x,Z_{i}) \to \int H(x,z)f(z|x)dz,
  \end{align*} where \(Z_{i} \stackrel{i.i.d}{\sim} f(z|x)\).\\
  If we approximate \(Q\) function using this idea, this then gives the
  so-called \textbf{Monte-Carlo EM}: \begin{align*}
    \hat{Q}(\theta|\theta^{(m)},\boldsymbol{x}) = \frac{1}{T}\sum^{T}_{j=1}log[L^{c}(\theta|\boldsymbol{x},\boldsymbol{z}_j)]. 
  \end{align*}\\
  where \(\boldsymbol{z}_1, \ldots, \boldsymbol{z}_T\) is an i.i.d.
  random sample generated from
  \(f(\boldsymbol{z}|\theta^{(m)}, \boldsymbol{x})\).
\item
  We may not need to find the exact maximizer in the process . Instead,
  sometimes we just find \(\theta^{(m+1)}\) that can improve upon the
  value of \(Q\) at the current \(\theta^{(m)}\), that is,
  \begin{align*}
    Q(\theta^{(m+1)}|\theta^{(m)},\boldsymbol{x}) \ge Q(\theta^{(m)}|\theta^{(m)},\boldsymbol{x}),  
  \end{align*} we called that \textbf{generalized EM Algorithm}.
\end{enumerate}

\hypertarget{em-algorithms-examples}{%
\subsubsection{EM Algorithms (examples)}\label{em-algorithms-examples}}

\hypertarget{e-step}{%
\subsection{E-step}\label{e-step}}

\[
Q(\theta\mid\theta^{(m)},\overrightharpoon{X})=E_{\theta^{(m)}}\big[\log L^c(\theta\mid\overrightharpoon{X},\overrightharpoon{Z})\mid\overrightharpoon{X}\big]
\]

where \(\overrightharpoon{X}\) is observable, \(\overrightharpoon{Z}\)
is unobservable; \(E_{\theta^{(m)}}\) is the conditional expectation of
\(Z\) given \(X\), where the conditional density is given by
\(f(\overrightharpoon{Z}\mid\overrightharpoon{X},\theta^{(m)})\);\\
\(\log L^c(\theta\mid\overrightharpoon{X},\overrightharpoon{Z})\) is
complete data likelihood function.

\hypertarget{m-step}{%
\subsection{M-step}\label{m-step}}

maximize \(\theta\to Q(\theta\mid\theta^{(m)},\overrightharpoon{X})\)\\
let
\(\theta^{(m+1)}\gets\mathop{\arg\max}\limits_{\theta}\ \ Q(\theta\mid\theta^{(m)},\overrightharpoon{X})\)

\hypertarget{two-component-mixture-of-normals}{%
\section{Two component mixture of
normals}\label{two-component-mixture-of-normals}}

Let
\(X_i\overset{iid}{\sim}\frac{1}{4}N(\mu_1,1)+\frac{3}{4}N(\mu_2,1)\),
\(\theta=(\mu_1,\mu_2)\)\\
log-likelihood function:
\[\log L(\theta\mid\overrightharpoon{X})=\sum\limits_{i=1}^{n}\log\big(\frac{1}{4}f(X_i\mid\mu_1)+\frac{3}{4}f(X_i\mid\mu_2)\big)\]
where
\(L(\theta\mid\overrightharpoon{X})=\prod\limits_{i=1}^{n}f(X_i\mid\mu_1,\mu_2)\)\\
The p.d.f of \(X_i\) is:\\
\[
f(X_i\mid \mu_j)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(X_i-\mu_j)^2}{2}}
\] Define some latent variables (binary):\\
\[P(Z_i=1)=\frac{1}{4}, P(Z_i=0)=\frac{3}{4}\] where
\(Z_i\perp(W_{1i},W_{2i})\)

If \(W_1\sim f_1\), \(W_2\sim f_2\), \(Z\sim Bernoulli(p)\) and
\(Z\perp(W_1,W_2)\), \(Z,W_1,W_2\) are independent. \[
X=ZW_1+(1-Z)W_2
\]

Complete data likelihood function
\((\overrightharpoon{X},\overrightharpoon{Z})\):\\
\[
\begin{aligned}
L^c=P(\overrightharpoon{X},\overrightharpoon{Z})&=\prod\limits_{i=1}^{n}P(X_i,Z_i)\\
&=\prod\limits_{i=1}^{n}\big[P(X_i\mid Z_i=1)P(Z_i=1)Z\big]^{Z_i}\big[P(X_i\mid Z_i=0)P(Z_i=0)\big]^{1-Z_i}
\end{aligned}
\]

So the log-likelihood function: \[
\log\big(L^c(\theta\mid\overrightharpoon{X}, \overrightharpoon{Z})\big)=\sum\limits_{i=1}^{n}\big[Z_i\log f(X_i\mid \mu_1)+Z_i\log\frac{1}{4}+(1-Z_i)\log f(X_i\mid \mu_2)+(1-Z_i)\log\frac{3}{4}\big]
\] Let \(\theta^{(0)}=(\mu_1^{(0)},\mu_2^{(0)})\)\\
\[
\begin{aligned}
Q(\theta\mid\theta^{(0)},\overrightharpoon{X})&=E_{\theta^{(0)}}\big[\log L^c(\theta\mid\overrightharpoon{X},\overrightharpoon{Z})\mid\overrightharpoon{X}\big]\\
&=-\sum\limits_{i=1}^{n}\Big\{\frac{1}{2}E_{\theta^{(0)}}[Z_i(X_i-\mu_1)^2\mid X_i]+\frac{1}{2}E_{\theta^{(0)}}\big[(1-Z_i)(X_i-\mu_2)^2\mid X_i\big]\Big\}\\
&\ \ \ \ \ +\sum\limits_{i=1}^{n}\Big\{\big(\log(\frac{1}{4})\big) E_{\theta^{(0)}}(Z_i\mid X_i)-\big( \log(\frac{3}{4}) \big) E_{\theta^{(0)}}(1-Z_i\mid X_i)\Big\}+C
\end{aligned}
\]

where \[
E_{\theta^{(0)}}[Z_i(X_i-\mu_1)^2\mid X_i]=(X_i-\mu_1)^2E_{\theta^{(0)}}[Z_i\mid X_i]
\] we can calculate
\[E[Z\mid X]=\frac{P(X\mid Z=1)P(Z=1)}{P(X=x\mid Z=1)P(Z=1)+P(X=x\mid Z=0)P(Z=0)}\]

\begin{mdframed}
Proof: 
\begin{align*}
E[Z\mid X]&= P(Z=1\mid X)\\
&=\frac{P(Z=1, X)}{P(X=x)}\\
&=\frac{P(X\mid Z=1)P(Z=1)}{P(X=x)}\\
&=\frac{P(X\mid Z=1)P(Z=1)}{P(X=x\mid Z=1)P(Z=1)+P(X=x\mid Z=0)P(Z=0)}
\end{align*}
\end{mdframed}

So we can calculate \(E_{\theta^{(0)}}[Z_i(X_i-\mu_1)^2\mid X_i]\):

\[
\begin{aligned}
E_{\theta^{(0)}}\big[Z_i(X_i-\mu_1)^2\mid X_i\big]&=\frac{\frac{1}{4}f_1(X_i\mid \mu_1^{(0)})}{\frac{1}{4}f_1(X_i\mid\mu_1^{(0)})+\frac{3}{4}f_2(X_i\mid\mu_2^{(0)})}\\
&=\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})
\end{aligned}
\]

And \(Q(\theta\mid\theta^{(0)},\overrightharpoon{X})\) can be expressed:
\[
Q(\theta\mid\theta^{(0)},\overrightharpoon{X})=-\sum\limits_{i=1}^{n}\Big\{\frac{1}{2}(X_i-\mu_1)^2\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})+\frac{1}{2}(X_i-\mu_2)^2\big(1-\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})\big)\Big\}+const
\]

Set the derivatives w.r.t. the \(\theta\) to zero function: \[
\begin{cases}
\frac{\partial Q(\theta\mid\theta^{(0)},\overrightharpoon{X})}{\partial \mu_1}\overset{set}{=}0\\
\frac{\partial Q(\theta\mid\theta^{(0)},\overrightharpoon{X})}{\partial \mu_2}\overset{set}{=}0
\end{cases}
\]

we can calculate the solutions \(\mu_1^{(1)}\) and \(\mu_2^{(1)}\)

\[
\begin{cases}
\mu_1^{(1)}=\frac{\sum\limits_{i=1}^{n}\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})X_i}{\sum\limits_{i=1}^{n}\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})}\\
\mu_2^{(1)}=\frac{\sum\limits_{i=1}^{n}\big[1-\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})\big]X_i}{\sum\limits_{i=1}^{n}\big[1-\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})\big]}
\end{cases}
\]

\hypertarget{right-censored-data-r.c-example-5.13-and-5.14}{%
\section{Right censored data (R.C example 5.13 and
5.14)}\label{right-censored-data-r.c-example-5.13-and-5.14}}

Let \(X_i\overset{iid}{\sim}f(x-\theta)\), where \(f\) is density
function of \(N(0,1)\), \(F\) be the CDF of \(N(0,1)\), so
\(X_i\sim N(\theta,1)\)\\
The Goal is to estimate \(\theta\). However, \(X_i\) are not fully
observed. They are right censored. The actual observation are \(Y_i\).\\
Let (1) \(Y_i\) is observed \[ 
Y_i=
\begin{cases}
a \ \ \ if \ X_i\ge a \\
X_i \ \ \ if \ X_i\le a
\end{cases}
\] where \(a\) is fixed.\\
It implies \(Y_i=\min(X_i,a)\).\\
(2)\(\delta_i=I(X_i\le a)\) be the indicator for non-censoring, or
equivalently, for observing the actual \(X_i\);\\
(3)\(n\) be sample size.

Assume (1)\((Y_1, Y_2, \dots, Y_m)\) all less than \(a\),\\
(2)\((Y_{m+1}, Y_{m+2}, \dots, Y_n)\) all equal than \(a\).

So the observed data likelihood function: \[
\begin{aligned}
L(\theta\mid Y_1, Y_2, \dots, Y_n, \delta_1, \delta_2, \dots, \delta_n)&=\prod\limits_{i=1}^{n}\big(f(Y_i-\theta)\big)^{\delta_i}\big(P(Y_i=a)\big)^{1-\delta_i}\\
&=\prod\limits_{i=1}^{n}\big(f(Y_i-\theta)\big)^{\delta_i}\big(1-P(X_i\le a)\big)^{1-\delta_i}\\
&=\prod\limits_{i=1}^{n}\big(f(Y_i-\theta)\big)^{\delta_i}\big(1-F(a-\theta)\big)^{1-\delta_i}
\end{aligned}
\]

Since \((Y_1, Y_2, \dots, Y_m)\) are uncensored,
\((Y_{m+1}, Y_{m+2}, \dots, Y_n)\) are censored.\\
Let \(\overrightharpoon{Z}\) be the vector of the unobservable
\(X_{m+1}, X_{m+2}, \dots, X_n\).\\
The complete data likelihood function: \[
L^c(\theta\mid Y_1, Y_2, \dots, Y_m; \overrightharpoon{Z})=\prod\limits_{i=1}^{m}f(Y_i-\theta)\prod\limits_{i=m+1}^{n}f(X_i-\theta)
\] The log-likelihood function: \[
\log L^c(\theta\mid Y_1, Y_2, \dots, Y_m; \overrightharpoon{Z})=\sum\limits_{i=1}^{m}\log f(Y_i-\theta)+\sum\limits_{i=m+1}^{n}\log f(X_i-\theta)
\]

\(Q(\theta\mid\theta^{(0)},\overrightharpoon{X})\) can be expressed: \[
\begin{aligned}
Q(\theta\mid\theta^{(0)};Y_1, \dots, Y_n, \delta_1, \dots, \delta_n)&=E_{\theta^{(0)}}\big[\log L^c(\theta\mid X_1, \dots, X_n)\mid Y_1, \dots, Y_n, \delta_1, \dots, \delta_n\big]\\
&=-\frac{1}{2}\sum\limits_{i=1}^{m}(Y_i-\theta)^2-\frac{1}{2}\sum\limits_{i=m+1}^{n}E_{\theta^{(0)}}\big[(Z_i-\theta)^2\mid Y_1, \dots, Y_n, \delta_1, \dots, \delta_n\big]
\end{aligned}
\]

For those \(i=m+1, \dots, n\), \(\delta_i=0\), So\\
\[
E_{\theta^{(0)}}\big[(Z_i-\theta)^2\mid Y_i, \delta_i\big]=E_{\theta^{(0)}}\big[(X_i-\theta)^2\mid X_i\ge a \big]
\]

We can calculate \(\theta^{(1)}\) by: \[
\frac{\partial Q(\theta\mid\theta^{(0)};Y_1, \dots, Y_n, \delta_1, \dots, \delta_n)}{\partial\theta}\overset{set}{=}0
\]

Which can be expressed: \[
\sum\limits_{i=1}^{m}(Y_i-\theta)+\sum\limits_{i=m+1}^{n}E_{\theta^{(0)}}\big[(X_i-\theta)\mid X_i\ge a\big]\overset{set}{=}0  \\
\] \[
\sum\limits_{i=1}^{m}Y_i-m\theta+\sum\limits_{i=m+1}^{n}E_{\theta^{(0)}}[X_i\mid X_i\ge a]-(n-m)\theta\overset{set}{=}0 
\]

we can calculate the solution \(\theta^{(1)}\): \[
\theta^{(1)}=\frac{\sum\limits_{i=1}^{m}Y_i+(n-m)E_{\theta^{(0)}}[X_i\mid X_i\ge a]}{n}
\]

Finally, we can show that \[
E_{\theta^{(0)}}[X_i\mid X_i\ge a]=\theta^{(0)}+\frac{\phi(a-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}
\]

\begin{mdframed}
Proof: 
The conditional density of $X_i$ given $X_i\ge a$, $X_i \sim f(x-\theta)$, $f\sim N(0,1)$

$$
f(X_i=x\mid X_i\ge a)=\frac{\phi(x-\theta)}{1-\Phi(a-\theta)}\ \ \ \ x>a
$$
The last thing is to compute the expectation. 
$$
\begin{aligned}
E_{\theta^{(0)}}[X_i\mid X_i\ge a]&=\int_{a}^{\infty}x\frac{\phi(x-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}\,dx\\
&=\int_{a}^{\infty}(\theta^{(0)}+x-\theta^{(0)})\frac{\phi(x-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}\,dx\\
&=\theta^{(0)}+\int_{a}^{\infty}(x-\theta^{(0)})\frac{\phi(x-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}\,dx\\
&\overset{w=x-\theta^{(0)}}{=}\theta^{(0)}+\frac{\int_{a-\theta^{(0)}}^{\infty}w\phi(w)\,dw}{1-\Phi(a-\theta^{(0)})}\\
&=\theta^{(0)}+\frac{-\phi(w)\Big|_{a-\theta^{(0)}}^{\infty}}{1-\Phi(a-\theta^{(0)})}\\
&=\theta^{(0)}+\frac{\phi(a-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}
\end{aligned}
$$
\end{mdframed}

\hypertarget{missing-data}{%
\subsubsection{Missing Data}\label{missing-data}}

\begin{itemize}
\tightlist
\item
  \(Z\) full data: \(Z=\left(Z_1, \cdots, Z_p\right)\).
\item
  \(R\) observation indicator
  \(R=\left(R_1, \cdots, R_p\right) \in \{0,1\}^p\) such that
  \begin{align*}
  R_k= \begin{cases}1 & \text { if } Z_k \text { is observed } \\ 0 & \text { if } Z_k \text { is missing }\end{cases}
  \end{align*} For a specific
  \(r=\left(r_1, \ldots, r_p\right) \in\{0,1\}^p\), \(Z_{(r)}\) is the
  subset of components that are observed; \(Z_{(\bar{r})}\) subset of
  components that are missing.
\item
  Let \(Z_{(R)}=\sum_r Z_{(r)} I(R=r)\). Similarly,
  \(Z_{\bar{R}}=\sum_{r} Z_{(r)} I(R \neq r)\).
\item
  The ideal full data is \((R, Z)\). The observed data is
  \(\left(R, Z_{(R)}\right)\).

  \begin{itemize}
  \tightlist
  \item
    A random sample of observed data is
    \(\left(R_i, Z_{(R_i)i}\right)_{i=1}^n.\)
  \end{itemize}
\end{itemize}

\textbf{MAR} (missing at random) assumption: \begin{align*}
P(R=r \mid Z)=P(R=r \mid Z_{(r)})
\end{align*}

\textbf{Selection model factorization}:

\begin{align*}
\begin{aligned}
P(z, r ; \psi, \theta) & =P(Z=z, R=r ; \psi, \theta) \\
& =P(R=r|Z=z ; \psi) P(Z=z ; \theta) \\
 & =P(R=r| Z_{(r)}=z_{(r)} ; \psi) P(Z=z ; \theta)
\end{aligned}
\end{align*} where the last equation is due to MAR. This factorization
(with MAR) is called separability condition, which says that there is no
information about \(\theta\) in \(\psi.\) Therefore, for the purpose of
estimation of \(\theta\), it suffices to consider \(P(Z=z ; \theta).\)

The likelihood for the observed data point
\(\left(R=r, Z_{(R)}=z_{(r)}\right)\) is given by \begin{align*}
P\left(R=r, z_{(R)}\right. & \left.=z_{(r)} ; \psi, \theta\right)=\int P(z ; r ; \psi, \theta) d z_{(\bar{r})} \\
& =P\left(R=r \mid Z_{(r)}=z_{(r)} ; \psi\right) \int P(z ; \theta) d z_{(\bar{r})} \\
& =P\left(R=r \mid Z_{(r)}=z_{(r)} ; \psi\right) P\left(Z_{(r)}=z_{(r)} ; \theta\right)
\end{align*}

The likelihood based on
\(\left.\left(R_i=r_i, z_{\left(R_i\right)i}=z_{\left(r_i\right)}\right)\right)_{i=1}^n\)
is given by

\begin{align*}
& \prod_i \prod_r P\left(R_i=r ; z_{(r) i}=z_{(r)} ; \psi, \theta\right)^{I\left(R_i=r\right)} \\
= & \prod_i\left(\prod_r P\left(R_i=r|Z_{(r) i}=z_{(r) ;} ; \psi \right)^{I\left(R_i=r\right)} \prod_r P\left(Z_{(r) i}=z_{(r)} ; \theta\right)^{I\left(R_i=r\right)}\right).
\end{align*} It suffices to maximize the following objective function
over \(\theta\): \begin{align*}
&\sum_{r=1}^n \sum_r I\left(R_i=r\right) \log P\left(Z_{(r)i}=z_{(r)}; \theta\right) \\
=&\sum_{i=1}^n \sum_r \left[I\left(R_i=r\right) \log \left( \int P\left(z_{(r)i}, z_{(\bar{r})i}; \theta\right) d{z_{(\bar{r}})}\right)\right].
\end{align*}

The E-step:

\begin{align*}
Q\left(\theta, \theta^{t-1}\right)=\sum_{i=1}^n E_{\theta^{t-1}}(\log P(Z_i;\theta)|R_i, Z_{(R_i)i})= \sum_{i=1}^n \sum_{r}I\left(R_i=r\right) E_{\theta^{t-1}}\left(\log P\left(Z_i; \theta\right) \mid Z_{(r) i}\right)
\end{align*} The secon equality holds because \begin{align*}
& \left.E_{\theta'}\left(\log P\left(Z_i; \theta\right) \mid R_i, Z_{(R_i)i}\right)\right) \\
= & \sum_{r}I\left(R_i=r\right) E_{\theta^{\prime}}\left(\log P\left(Z_i; \theta\right) \mid R_i=r, Z_{(r)i}\right) \\
= &  \sum_{r}I\left(R_i=r\right) E_{\theta^{\prime}}\left(\log P\left(Z_i; \theta\right) \mid Z_{(r) i}\right)
\end{align*} here the expectation is conditional on \((R_i, Z_{(r)i})\)
with parameter \(\theta'\). The last equality holds because of MAR.

The M-step:

\[\theta^t =\arg \max_{\theta} Q(\theta, \theta^{t-1}).\]

\textbf{Example}: Suppose \(Z\) follows a \(N(\mu,\Sigma)\), here
\(\theta:=(\mu,\Sigma)\). Note that \begin{align*}
&E_{\theta^{t-1}}\left(\log P\left(Z_i; \theta\right) \mid Z_{(r) i}\right) \\ =&-\frac{1}{2} \log |2 \pi {\Sigma}|-\frac{1}{2}  {E}_{\theta^{t-1}}\left[\left(Z_i-{\mu}\right)^{\top} {\Sigma}^{-1}\left(Z_i-{\mu}\right)\right] \\
=&-\frac{1}{2} \log |{\Sigma}|-\frac{p}{2} \log (2 \pi)-\frac{1}{2} \operatorname{tr}\left({\Sigma}^{-1} {E}_{\theta^{t-1}}[\mathbf{S}({\mu})]\right)
\end{align*} where
\({E}_{\theta^{t-1}}[\mathbf{S}({\mu})]:= \left({E}_{\theta^{t-1}}\left[Z_i Z_i^{\top}\right]+{\mu} {\mu}^{\top}-2 {\mu} {E}_{\theta^{t-1}}\left[Z_i\right]^{\top}\right).\)
The key is to compute \({E}_{\theta^{t-1}}\left[Z_i\right]\) and
\({E}_{\theta^{t-1}}\left[Z_i Z_i^{\top}\right]\) (expected sufficient
statistics) and they can be computed using conditional formula for
multivariate Gaussian distribution. The solution in the M-step then is
given by the MLE solution with the expected sufficient statistics:
\begin{align*}
{\mu}^t & ={E}_{\theta^{t-1}}\left[Z_i\right] \\
{\Sigma}^t & ={E}_{\theta^{t-1}}\left[Z_i Z_i^{\top}\right]-{\mu}^t\left({\mu}^t\right)^{\top}.
\end{align*}

{ { \href{javascript:window.history.back()}{Back} } { \textbf{Updated:}
2023-09-27 } { Statistical Simulation, Wei Li } }

\end{document}
