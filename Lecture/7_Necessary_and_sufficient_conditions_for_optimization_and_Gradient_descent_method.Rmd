---
title: "MAT 653: Statistical Simulation"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Yuan Liu
date: Sep 23th  , 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  pdf_document: 
      latex_engine: xelatex
      include:
        in_header: preamble_rmd.tex
  html_document:
    df_print: paged
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>




# Necessary Conditions

## (a) First-Order necessary condition
**Fact**: Let f be continuously differentiable, if $x^*$  is a minimizer of f, then $\nabla f(x^*) = 0$ .
  
## (b) Second-order necessary condition
**Fact**: let f be continuously twice differentiable, if $x^*$ is a minimizer of f, then $\nabla f(x^*) = 0$ , and  $\nabla^2 f(x^*) \geq 0$ .
      
# Sufficient Conditions(2nd order condition)
**Fact**: suppose $\nabla^2 f(x)$ is continuous and at some point $x^*$, if $\nabla^2 f(x^{*}) > 0$, and $\nabla f(x^*) = 0$
        $\Rightarrow$ then $x^*$ is a minimizer(strict local).

# Convex function and global minimizer

**Fact**: when f is convex on S, then any local minimizer $x^*$ is a global minimizer.

If in addition, f is differentiable, then any stationary point is a global minimizer.

f: $\mathbb{R^n} \rightarrow \mathbb{R}$
Hessian matrix at $x^*$, such that $\nabla f(x^*) = 0$

**1** If $\nabla^2 f(x^*) > 0$, then any local minimizer $x^*$ is a strict local minimizer.

**2** If $\nabla^2 f(x^*) < 0$, then local maximizer $x^*$ is a strict local maximizer.

**3** If $\nabla^2 f(x^*)$ is indefinite(i.e. neither positive semi-definite nor negative semi-definite), then the local minimizer $x^*$ is a saddle point.

**4** If those cases not listed above, then the test is inconclusive.

# Numerical method for optimization

$min f(x) \rightarrow x^*$

## Two methods:
1. Gradient method

2. Newton's method

Both guarantee that the stationary points fo f can be found (i.e. $\nabla f(x^*) = 0$).

## Steps
**1** start the process with some initial point $x_0$;

**2** then iterate steps denoted by $x_k \rightarrow x_{k+1}$ going downhill toward a stationary point of f.

Repeat **2** until the sequance of points converge to a stationary point.

For a general(non-convex) function, we run the procedure several times with diffrent initial values $x_0$.

# Gradient descent
Choose a direction $P_k$ and search along the direction from the current iterate $x_k$ for a new iterate with lower function value.

$f(x_{k+1}) < f(x_k)$

$x_k+\alpha P_k$ ( $\alpha$ = "step length", "learning rate",  $\alpha>0$ scalar)

Fix $\alpha$ : Taylor approximation of f at $x_k$

$f(x_k +\alpha p)$ $\approx$ $f(x_k)+\alpha p^T\nabla f(x_k)$

$\min_{||p||=1} P^T \nabla f(x_k):$ solution gives us the unit direction that is most rapid decrease. 

$p^T\nabla f(x_k) = ||p|| \cdot ||\nabla f(x_k)|| cos(\theta)$, $0\leq \theta \leq \pi$

$\Rightarrow cos(\theta) = -1$

$\Rightarrow$ p is the exact opposite direction of $\nabla f(x_k)$ , $p=\frac{-\nabla f(x_k)}{||\nabla f(x_k)||}$

**Any $P_k$ such that $P_k^T \nabla f(x_k)<0$ would work. It's called "descent direction".**

## Algorithm
Gradient descent (fix $\alpha$), set $k = 0$, given $x_0$

Repeat $x_{k+1} \leftarrow x_k - \alpha \nabla f(x_k)$, k $\leftarrow$ k+1

Until stopping condition is met ($||\nabla f(x_k)||=0$).