---
title: "MAT 653: Metropolis-Hastings Algorithm (discrete space)"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Peichen Yu  
date: Nov 4th, 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  pdf_document: 
      include:
        in_header: preamble_rmd.tex
  html_document:
    df_print: paged
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


Given a steady-state distribution $\pi$, how to find a M.C. that has $\pi$ as its steady-state distribution?

**Goal**: to find the right transition kernel $P_{ij}$

**Idea**: Consider a M.C. that is "time reversible", i.e. whose direction of the time does not matter in the dynanic of the chain. 

Farmally, $P(X^{(n)}=j|X^{(n+1)}=i)=P(X^{(n+1)}=j|X^{(n)}=i)$, for $\forall i,j$. 

Suppose we are at steady-state $\pi$,
$$
P(X^{(n)}=i|X^{(n+1)}=j)P(X^{(n+1)}=j)=P(X^{(n+1)}=j|X^{(n)}=i)P(X^{(n)}=i)
$$
by time reversibility, 
$$
P(X^{(n)}=i|X^{(n+1)}=j)=P(X^{(n+1)}=i|X^{(n)}=j)=P_{ji}
$$
imply that,
$$
\pi_jP_{ji}=\pi_iP_{ij}, \forall i,j 
$$
(detailed balance condition)

This says that the flows of probabilities between two states $i, j$ balance each other.

**Theorem**: A set of transition probabilities $P_{i,j}$, satisfying the "detailed balance condition" will have $\pi$ as its steady state distribution.

\begin{mdframed}
Proof: $\sum_i\pi_iP_{ij}=\sum_i\pi_jP_{ji}=\pi_j\sum_iP_{ji}=\pi_j \times 1=\pi_j$
\end{mdframed}


# Metropolis Algorithm

**Goal**: to find the transition kernel $\tilde{P}_{ij}$, s.t., the detailed balance condition holds, i.e.,  $\pi_j\tilde{P}_{ji}=\pi_i\tilde{P}_{ij},\forall i,j$.

We can use some arbitrary transition kernel, called ${P}_{ij}$. By following the modification described below, we can change $P_{ij} \rightarrow \tilde{P}_{ij}$. 

If $\pi_iP_{ij}<\pi_jP_{ji}$, we accept all the transition from i to j, but only some of the transition from j to i to bring back into balance.

If $\pi_iP_{ij}>\pi_jP_{ji}$, we are to bring back into balance by only accept some of the transition from i to j, but accept all of the transition from j to i.

Formally, Metropolis' idea works as follows: 

Given $\pi$, the steady-state distribution (target distribution). For all $i$, 

(1) Start from $P_{ij}$
(2) Define acceptance probability $\alpha_{ij}=min[\frac{\pi_jP_{ji}}{\pi_iP_ij},1]$ for each $j\neq i$. Let 
$$
\tilde{P}_{ij}=\alpha_{ij}P_{ij},  j \neq i
$$
$$
\tilde{P}_{ii}=1-\sum_{j:j\neq i}\tilde{P}_{ij}
$$

Then $\pi$ is the steady-state distribution from M.C. with transition probability given by this $\tilde{P}_{ij}$.

\begin{mdframed}
Proof: 
\end{mdframed}

**Note**: $\tilde{P}_{ii}=1-\sum_{j:j\neq i}\tilde{P}_{ij}$ is the probability that I reject any proposal that moves from i to any other state $j\neq i$, that is, it is the probability of moving from i to i itself. We can prove that our $\tilde{P}_{ij}$ is well-defined transition kernel.
\begin{mdframed}
Check: $1\geq \tilde{P}_{ij}\geq0$, $\sum_j\tilde{P}_{ij}=\sum_j\tilde{P}_{ij}=\tilde{P}_{ii}+\sum_{j:j\neq i}\tilde{P}_{ij}=1-\sum_{j:j\neq i}\tilde{P}_{ij}+\sum_{j:j\neq i}\tilde{P}_{ij}=1$
\end{mdframed}

The following algorithm implements above procedure. 

**Algorithm (Metropolis)**: $X^{(0)}\in S$, $S$ is a discrete space.

1. draw $X^* \sim P(\cdot|X^{(t-1)})$

2. compute acceptance probability
$$
\alpha(X^{(t-1)},X^*):=min\left\{1,\frac{P(X^{(t-1)}|X^*)\pi_{X^*}}{P(X^*|X^{(t-1)})\pi_{X^{(t-1)}}}\right\}
$$
where $\pi_{X^*}$ is the probability of taking value $X^*$ under the steady-state distribution $\pi$, and $\pi_{X^{(t-1)}}$ is the probability of taking value $X^{(t-1)}$ under the steady-state distribution $\pi$. 
3. with probability $\alpha(X^{(t-1)},X)$, set $X^{(t)}\leftarrow X^*$; otherwise $X^{(t)}\leftarrow X^{(t-1)}$.

To implement step 3, one can implement: generate U~Unif(0,1), if $U \leq \alpha(X^{(t-1)},X^*)$, then accept $X^*$, i.e., set $X^{(t)}\leftarrow X^*$; otherwise, reject $X^*$, set  $X^{(t)}\leftarrow X^{(t-1)}$.