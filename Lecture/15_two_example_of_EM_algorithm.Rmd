---
title: "MAT 653: Statistical Simulation"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Meng He  
date: Oct 21th  , 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  html_document:
    df_print: paged
  pdf_document: 
      latex_engine: xelatex
      include:
        in_header: preamble_rmd.tex
        extra_dependencies: ["fdsymbol"]
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \usepackage{mathtools}
- \usepackage{fdsymbol}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


# EM Algorithms
## E-step
$$
Q(\theta\mid\theta^{(m)},\overrightharpoon{X})=E_{\theta^{(m)}}\big[\log L^c(\theta\mid\overrightharpoon{X},\overrightharpoon{Z})\mid\overrightharpoon{X}\big]
$$

where $\overrightharpoon{X}$ is observable, $\overrightharpoon{Z}$ is unobservable; $E_{\theta^{(m)}}$ is the conditional expectation of $Z$ given $X$, where the conditional density is given by $f(\overrightharpoon{Z}\mid\overrightharpoon{X},\theta^{(m)})$;  
$\log L^c(\theta\mid\overrightharpoon{X},\overrightharpoon{Z})$ is complete data likelihood function.



## M-step
maximize $\theta\to Q(\theta\mid\theta^{(m)},\overrightharpoon{X})$  
let $\theta^{(m+1)}\gets\mathop{\arg\max}\limits_{\theta}\ \ Q(\theta\mid\theta^{(m)},\overrightharpoon{X})$  

# Two component mixture of normals
Let $X_i\overset{iid}{\sim}\frac{1}{4}N(\mu_1,1)+\frac{3}{4}N(\mu_2,1)$, $\theta=(\mu_1,\mu_2)$  
log-likelihood function:
$$\log L(\theta\mid\overrightharpoon{X})=\sum\limits_{i=1}^{n}\log\big(\frac{1}{4}f(X_i\mid\mu_1)+\frac{3}{4}f(X_i\mid\mu_2)\big)$$ 
where $L(\theta\mid\overrightharpoon{X})=\prod\limits_{i=1}^{n}f(X_i\mid\mu_1,\mu_2)$  
The p.d.f of $X_i$ is:  
$$
f(X_i\mid \mu_j)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(X_i-\mu_j)^2}{2}}
$$
Define some latent variables (binary):  
$$P(Z_i=1)=\frac{1}{4}, P(Z_i=0)=\frac{3}{4}$$
where $Z_i\perp(W_{1i},W_{2i})$  

If $W_1\sim f_1$, $W_2\sim f_2$, $Z\sim Bernoulli(p)$ and $Z\perp(W_1,W_2)$, $Z,W_1,W_2$ are independent.
$$
X=ZW_1+(1-Z)W_2
$$


Complete data likelihood function $(\overrightharpoon{X},\overrightharpoon{Z})$:  
$$
\begin{aligned}
L^c=P(\overrightharpoon{X},\overrightharpoon{Z})&=\prod\limits_{i=1}^{n}P(X_i,Z_i)\\
&=\prod\limits_{i=1}^{n}\big[P(X_i\mid Z_i=1)P(Z_i=1)Z\big]^{Z_i}\big[P(X_i\mid Z_i=0)P(Z_i=0)\big]^{1-Z_i}
\end{aligned}
$$

So the log-likelihood function:
$$
\log\big(L^c(\theta\mid\overrightharpoon{X}, \overrightharpoon{Z})\big)=\sum\limits_{i=1}^{n}\big[Z_i\log f(X_i\mid \mu_1)+Z_i\log\frac{1}{4}+(1-Z_i)\log f(X_i\mid \mu_2)+(1-Z_i)\log\frac{3}{4}\big]
$$
Let $\theta^{(0)}=(\mu_1^{(0)},\mu_2^{(0)})$  
$$
\begin{aligned}
Q(\theta\mid\theta^{(0)},\overrightharpoon{X})&=E_{\theta^{(0)}}\big[\log L^c(\theta\mid\overrightharpoon{X},\overrightharpoon{Z})\mid\overrightharpoon{X}\big]\\
&=-\sum\limits_{i=1}^{n}\Big\{\frac{1}{2}E_{\theta^{(0)}}[Z_i(X_i-\mu_1)^2\mid X_i]+\frac{1}{2}E_{\theta^{(0)}}\big[(1-Z_i)(X_i-\mu_2)^2\mid X_i\big]\Big\}\\
&\ \ \ \ \ +\sum\limits_{i=1}^{n}\Big\{\big(\log(\frac{1}{4})\big) E_{\theta^{(0)}}(Z_i\mid X_i)-\big( \log(\frac{3}{4}) \big) E_{\theta^{(0)}}(1-Z_i\mid X_i)\Big\}+C
\end{aligned}
$$


where
$$
E_{\theta^{(0)}}[Z_i(X_i-\mu_1)^2\mid X_i]=(X_i-\mu_1)^2E_{\theta^{(0)}}[Z_i\mid X_i]
$$
we can calculate 
$$E[Z\mid X]=\frac{P(X\mid Z=1)P(Z=1)}{P(X=x\mid Z=1)P(Z=1)+P(X=x\mid Z=0)P(Z=0)}$$
\begin{mdframed}
Proof: 
\begin{align*}
E[Z\mid X]&= P(Z=1\mid X)\\
&=\frac{P(Z=1, X)}{P(X=x)}\\
&=\frac{P(X\mid Z=1)P(Z=1)}{P(X=x)}\\
&=\frac{P(X\mid Z=1)P(Z=1)}{P(X=x\mid Z=1)P(Z=1)+P(X=x\mid Z=0)P(Z=0)}
\end{align*}
\end{mdframed}

So we can calculate $E_{\theta^{(0)}}[Z_i(X_i-\mu_1)^2\mid X_i]$:

$$
\begin{aligned}
E_{\theta^{(0)}}\big[Z_i(X_i-\mu_1)^2\mid X_i\big]&=\frac{\frac{1}{4}f_1(X_i\mid \mu_1^{(0)})}{\frac{1}{4}f_1(X_i\mid\mu_1^{(0)})+\frac{3}{4}f_2(X_i\mid\mu_2^{(0)})}\\
&=\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})
\end{aligned}
$$

And $Q(\theta\mid\theta^{(0)},\overrightharpoon{X})$ can be expressed:
$$
Q(\theta\mid\theta^{(0)},\overrightharpoon{X})=-\sum\limits_{i=1}^{n}\Big\{\frac{1}{2}(X_i-\mu_1)^2\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})+\frac{1}{2}(X_i-\mu_2)^2\big(1-\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})\big)\Big\}+const
$$

Set the derivatives w.r.t. the $\theta$ to zero function:
$$
\begin{cases}
\frac{\partial Q(\theta\mid\theta^{(0)},\overrightharpoon{X})}{\partial \mu_1}\overset{set}{=}0\\
\frac{\partial Q(\theta\mid\theta^{(0)},\overrightharpoon{X})}{\partial \mu_2}\overset{set}{=}0
\end{cases}
$$


we can calculate the solutions $\mu_1^{(1)}$ and $\mu_2^{(1)}$


$$
\begin{cases}
\mu_1^{(1)}=\frac{\sum\limits_{i=1}^{n}\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})X_i}{\sum\limits_{i=1}^{n}\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})}\\
\mu_2^{(1)}=\frac{\sum\limits_{i=1}^{n}\big[1-\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})\big]X_i}{\sum\limits_{i=1}^{n}\big[1-\alpha_i^{(0)}(X_i;\mu_1^{(0)},\mu_2^{(0)})\big]}
\end{cases}
$$

# Right censored data (R.C example 5.13 and 5.14)
Let $X_i\overset{iid}{\sim}f(x-\theta)$, where $f$ is density function of $N(0,1)$, $F$ be the CDF of $N(0,1)$, so $X_i\sim N(\theta,1)$  
The Goal is to estimate $\theta$. However, $X_i$ are not fully observed. They are right censored. The actual observation are $Y_i$.     
Let (1) $Y_i$ is observed 
$$ 
Y_i=
\begin{cases}
a \ \ \ if \ X_i\ge a \\
X_i \ \ \ if \ X_i\le a
\end{cases}
$$
where $a$ is fixed.   
It implies $Y_i=\min(X_i,a)$.  
(2)$\delta_i=I(X_i\le a)$ be the indicator for non-censoring, or equivalently, for observing the actual $X_i$;  
(3)$n$ be sample size.  
  
Assume 
(1)$(Y_1, Y_2, \dots, Y_m)$ all less than $a$,  
(2)$(Y_{m+1}, Y_{m+2}, \dots, Y_n)$ all equal than $a$.  



  
So the observed data likelihood function:
$$
\begin{aligned}
L(\theta\mid Y_1, Y_2, \dots, Y_n, \delta_1, \delta_2, \dots, \delta_n)&=\prod\limits_{i=1}^{n}\big(f(Y_i-\theta)\big)^{\delta_i}\big(P(Y_i=a)\big)^{1-\delta_i}\\
&=\prod\limits_{i=1}^{n}\big(f(Y_i-\theta)\big)^{\delta_i}\big(1-P(X_i\le a)\big)^{1-\delta_i}\\
&=\prod\limits_{i=1}^{n}\big(f(Y_i-\theta)\big)^{\delta_i}\big(1-F(a-\theta)\big)^{1-\delta_i}
\end{aligned}
$$

Since $(Y_1, Y_2, \dots, Y_m)$ are uncensored, $(Y_{m+1}, Y_{m+2}, \dots, Y_n)$ are censored.  
Let $\overrightharpoon{Z}$ be the vector of the unobservable $X_{m+1}, X_{m+2}, \dots, X_n$.  
The complete data likelihood function:
$$
L^c(\theta\mid Y_1, Y_2, \dots, Y_m; \overrightharpoon{Z})=\prod\limits_{i=1}^{m}f(Y_i-\theta)\prod\limits_{i=m+1}^{n}f(X_i-\theta)
$$
The log-likelihood function:
$$
\log L^c(\theta\mid Y_1, Y_2, \dots, Y_m; \overrightharpoon{Z})=\sum\limits_{i=1}^{m}\log f(Y_i-\theta)+\sum\limits_{i=m+1}^{n}\log f(X_i-\theta)
$$

$Q(\theta\mid\theta^{(0)},\overrightharpoon{X})$ can be expressed:
$$
\begin{aligned}
Q(\theta\mid\theta^{(0)};Y_1, \dots, Y_n, \delta_1, \dots, \delta_n)&=E_{\theta^{(0)}}\big[\log L^c(\theta\mid X_1, \dots, X_n)\mid Y_1, \dots, Y_n, \delta_1, \dots, \delta_n\big]\\
&=-\frac{1}{2}\sum\limits_{i=1}^{m}(Y_i-\theta)^2-\frac{1}{2}\sum\limits_{i=m+1}^{n}E_{\theta^{(0)}}\big[(Z_i-\theta)^2\mid Y_1, \dots, Y_n, \delta_1, \dots, \delta_n\big]
\end{aligned}
$$

For those $i=m+1, \dots, n$, $\delta_i=0$, So  
$$
E_{\theta^{(0)}}\big[(Z_i-\theta)^2\mid Y_i, \delta_i\big]=E_{\theta^{(0)}}\big[(X_i-\theta)^2\mid X_i\ge a \big]
$$


We can calculate $\theta^{(1)}$ by:
$$
\frac{\partial Q(\theta\mid\theta^{(0)};Y_1, \dots, Y_n, \delta_1, \dots, \delta_n)}{\partial\theta}\overset{set}{=}0
$$

Which can be expressed:
$$
\sum\limits_{i=1}^{m}(Y_i-\theta)+\sum\limits_{i=m+1}^{n}E_{\theta^{(0)}}\big[(X_i-\theta)\mid X_i\ge a\big]\overset{set}{=}0  \\
$$
$$
\sum\limits_{i=1}^{m}Y_i-m\theta+\sum\limits_{i=m+1}^{n}E_{\theta^{(0)}}[X_i\mid X_i\ge a]-(n-m)\theta\overset{set}{=}0 
$$

we can calculate the solution $\theta^{(1)}$:
$$
\theta^{(1)}=\frac{\sum\limits_{i=1}^{m}Y_i+(n-m)E_{\theta^{(0)}}[X_i\mid X_i\ge a]}{n}
$$

Finally, we can show that 
$$
E_{\theta^{(0)}}[X_i\mid X_i\ge a]=\theta^{(0)}+\frac{\phi(a-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}
$$
\begin{mdframed}
Proof: 
The conditional density of $X_i$ given $X_i\ge a$, $X_i \sim f(x-\theta)$, $f\sim N(0,1)$

$$
f(X_i=x\mid X_i\ge a)=\frac{\phi(x-\theta)}{1-\Phi(a-\theta)}\ \ \ \ x>a
$$
The last thing is to compute the expectation. 
$$
\begin{aligned}
E_{\theta^{(0)}}[X_i\mid X_i\ge a]&=\int_{a}^{\infty}x\frac{\phi(x-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}\,dx\\
&=\int_{a}^{\infty}(\theta^{(0)}+x-\theta^{(0)})\frac{\phi(x-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}\,dx\\
&=\theta^{(0)}+\int_{a}^{\infty}(x-\theta^{(0)})\frac{\phi(x-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}\,dx\\
&\overset{w=x-\theta^{(0)}}{=}\theta^{(0)}+\frac{\int_{a-\theta^{(0)}}^{\infty}w\phi(w)\,dw}{1-\Phi(a-\theta^{(0)})}\\
&=\theta^{(0)}+\frac{-\phi(w)\Big|_{a-\theta^{(0)}}^{\infty}}{1-\Phi(a-\theta^{(0)})}\\
&=\theta^{(0)}+\frac{\phi(a-\theta^{(0)})}{1-\Phi(a-\theta^{(0)})}
\end{aligned}
$$
\end{mdframed}



