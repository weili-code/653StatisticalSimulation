---
title: "MAT 653: Statistical Simulation"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Meng He  
date: Sep 22th  , 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  pdf_document: 
      latex_engine: xelatex
      include:
        in_header: preamble_rmd.tex
  html_document:
    df_print: paged
  html_notebook: default
fontsize: 14pt
header-includes:
- \usepackage{amsmath}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \usepackage{mathtools}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


# Basics of vector calculus

## Gradient

For $f:{\mathbb{R}^n}\to{\mathbb{R}}$, its gradient is $n{\times}1$ matrix.
$$
{\nabla}f(x)=\begin{bmatrix}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_n} \\ 
\end{bmatrix}
$$


## Hessian matrix

The Hessian matrix of $f$ if square $n{\times}n$ matrix, is defined
$$
{\nabla}^2f(x)=\begin{bmatrix}
{\frac{\partial^2 f(x)}{\partial x_1 \partial x_1}}& {\frac{\partial^2 f(x)}{\partial x_2 \partial x_1}}& \cdots & {\frac{\partial^2 f(x)}{\partial x_n \partial x_1}}\\
\vdots & \vdots & \ddots & \vdots \\
{\frac{\partial^2 f(x)}{\partial x_1 \partial x_n}}& {\frac{\partial^2 f(x)}{\partial x_2 \partial x_n}}& \cdots & {\frac{\partial^2 f(x)}{\partial x_n \partial x_n}}\\ 
\end{bmatrix}
$$



\begin{mdframed}
Example: 
(1)
$f(x)=a^Tx :{\mathbb{R^n}}\to{\mathbb{R}}$, $f(x)$ can also written in $f(x)={\sum_{i=1}^{n}}a_ix_i$,
the gradient for $f(x)$,
$${\nabla}f(x)=a$$
(2)
$f(x)=x^TAx$ ($A$ is square matrix), the gradient for $f(x)$,
$$
{\nabla}f(x)=Ax+A^Tx
$$
if A is symmetric matrix,
$$
{\nabla}f(x)=2Ax
$$
\end{mdframed}
 
 
\begin{mdframed}
Proof:
\end{mdframed}

For the linear least square problem$\min\limits_{x}{f(x)}=\min\limits_{x}{\lVert Ax-b\rVert^2}$,

$$
\begin{aligned}
f(x)&=\lVert Ax-b\rVert^2\\&=\langle Ax-b, Ax-b\rangle\\&=(Ax-b)^T(Ax-b)
\\&=x^TA^TAx-2b^TAx+b^Tb
\end{aligned}
$$

the gradient of $f(x)$ is
$$
{\nabla}f(x)=\frac{{\partial}f(x)}{{\partial}x}=2A^TAx-2b^TA\overset{set}{=}0
$$

# Optimization
reference: Numerical Optimization (nocedal and wright)

## Definition


Given $f:{\mathbb{R^n}}\to{\mathbb{R}}$, $S{\subseteq}{\mathbb{R^n}}$ is feasible set.

**1.Global minimizer:**

A point $x^*{\in}S$ is a global minimizer of $f$ in $S$ if and only if $f(x^*){\le}f(x)$ for all $x{\in}S$

**2.Local minimizer:**

A point $x^*{\in}S$ is a local minimizer of $f$ in $S$ if and only if there exists a neighborhood of $x^*$, $N(x^*)$ such that $f(x^*){\le}f(x)$, for all $x{\in}N(x^*){\cap}S$

**3.Strict local minimizer:**

A point $x^*{\in}S$ is a local minimizer of $f$ in $S$ if and only if there exists a neighborhood of $x^*$, $N(x^*)$ such that $f(x^*)<f(x)$, for all $x{\in}N(x^*){\cap}S$

**4.Convex set:**

$S$ is a convex set if and only if $x,y{\in}S$ entails ${\alpha}x+(1-{\alpha})y{\in}S$ for all ${\alpha}{\in}[0,1]$

**5.Convex functions**

A function $f$ is convex on $S$ if and only if it's convex at every point of $S$.

(1) A "smooth" function $f$ is a convex at the point $v$ if and only if ${\nabla}^2f(v){\ge}0$  (positive semidefinite). 
(if $f:{\mathbb{R}}\to{\mathbb{R}}$, $\left. \frac{{\partial}^2f(x)}{{\partial}x^2}\right|_{x=v}{\ge}0$ ).

$\frac{{\partial}f(v)}{{\partial}x}$: slope at $v$ = rate of change of $f$ at $v$.

$\frac{{\partial}^2f(v)}{{\partial}x^2}=$  rate of change of the slope of $f$ at $v$.

(2) A "differentialbe" function $f$ is convex at $v$ if and only if for all $w$, $f(w){\ge}f(v)+{\nabla}f(v)(w-v)$ (when $w$ is neighborhood of $v$).

(3) A function $f$ is convex on $S$, if and only if $x,y{\in}S$ entails $f({\alpha}x+(1-{\alpha})y){\le}{\alpha}f(x)+(1-{\alpha})f(y)$ for ${\alpha}{\in}[0,1]$.

**6.First order condition**

FOC (first order condition): ${\nabla}f(x)\overset{set}{=}0$

**7.Saddle point**

A saddle point is a stationary point at which the curvature of the function changes from negative to positive or vice versa.

**9.Stationary point**

A stationary point $v$ of $f$ satisfies the FOC ${\nabla}f(x){=}0$, $f:{\mathbb{R}^n}\to{\mathbb{R}}$. A stationary point could be  minimizer, maximizer or saddle point.


