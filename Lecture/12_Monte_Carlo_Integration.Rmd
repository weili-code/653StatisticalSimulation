---
title: "MAT 653: Statistical Simulation"
author:
  - Instructor$:$ Dr. Wei Li
  - Scribe$:$ Jianqing Jia   
date: Oct 12th, 2021
geometry: left=2cm, right=2cm, top=2cm, bottom=2cm
linestretch: 1.3
output:
  pdf_document: 
      include:
        in_header: preamble_rmd.tex
  html_notebook: default
  html_document:
    df_print: paged
fontsize: 14pt
header-includes:
- \usepackage{amsmath,mathrsfs,mathtools}
- \usepackage[linewidth=1pt]{mdframed}
- \newcommand\inner[2]{\left\langle#1,#2\right\rangle}
- \newcommand\floor[1]{\lfloor#1\rfloor}
- \newcommand\ceil[1]{\lceil#1\rceil}
- \newcommand\mb[1]{\mathbf{#1}}
- \newcommand\bs[1]{\boldsymbol{#1}}
- \newcommand\mr[1]{\mathrm{#1}}
- \newcommand\wh[1]{\widehat{#1}}
- \newcommand\op[1]{\operatorname{#1}}
- \newcommand\mbb[1]{\mathbb{#1}}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \newcommand{\wei}[1]{\textcolor{orange}{(Wei:#1)}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


# Monte Carlo Integration

*Reading: R.C. Section 3.1-3.3 $\&$ Voss Section 3.1-3.3*

## Classical Monte Carlo Integration

**General problem** is to evaluate 
$$
\mathbb{E}_f [h(X)]=\int_{\mathcal{X}} h(x) f(x)\,dx
$$

where $h$ is some known function, $X$ is r.v. $\sim$ $f$ and $\mathcal{X}=\textit{supp} (f)$. In the following discussion, sometimes the underlying $\mathcal{X}$ is implicitly assumed.

The principle of M.C. method for evaluating this is to generate an i.i.d. random sample sequence $(X_1, \cdots, X_n)$ from $f$ and propose to estimate $\mathbb{E}_f [h(X)]$ by empirical average
$$
\overline{h}_n=\frac{1}{n}\sum_{j=1}^n h(X_j)
$$
this is called **M.C. estimator for $\mathbb{E}_f [h(X)]$**. 

If you have a realization for $\overline h _n =\frac{1}{n}\sum_{j=1}^n h(X_j)$, then this gives you an estimate for $\mathbb{E}_f [h(X)]$. This works because by Law of Large numbers, $\overline h_n \rightarrow \mathbb{E}_f [h(X)]$.

**Properties of estimator $\overline h_n$**

**1.** $\mathbb{E}_f (\overline h_n)=\mathbb{E}_f [h(X)]$ (i.e. bias=0)

\begin{mdframed}
Proof: $\mathbb{E}_f (\overline h_n)= \frac{1}{n} \sum_{j=1}^n \mathbb{E}_f[h(X_j)]=\frac{1}{n} \sum_{j=1}^n \mathbb{E}_f[h(X)]=\mathbb{E}_f [h(X)]$
\end{mdframed}

**2.** $\mathbb{V}ar_f (\overline h_n)=\displaystyle \frac{1}{n}\mathbb{V}ar_f [h(X)]$

\begin{mdframed}
Proof: 
\begin{align*}
\mathbb{V}ar_f (\overline h_n)&= \mathbb{V}ar_f \big[\frac{1}{n}\sum_{j=1}^n h(X_j) \big]= \frac{1}{n^2} \mathbb{V}ar_f \big[ \sum_{j=1}^n h(X_j) \big]\\
&\overset{(*)}=\frac{1}{n^2} \sum_{j=1}^n \mathbb{V}ar_f [ h(X_j)]=\frac{1}{n^2} \sum_{j=1}^n \mathbb{V}ar_f [h(X)]= \frac{1}{n}\mathbb{V}ar_f [h(X)]
\end{align*}
where $(*)$ holds since $X_j \overset{iid}\sim f$, $Cov(X_i,X_j)=0\quad (i\ne j)$
\end{mdframed}

And $\mathbb{V}ar_f (\overline h_n)$ can also be estimated from $(X_1, \cdots, X_n)$ by $\displaystyle\frac{1}{n^2} \sum_{j=1}^n \big[h(X_j)-\frac{1}{n}\sum_{i=1}^n h(X_i)\big]^2$.

\begin{mdframed}
Proof: 
\begin{align*}
\mathbb{V}ar_f (\overline h_n)&=\frac{1}{n}\mathbb{V}ar_f [h(X)]\\
&=\frac{1}{n}\mathbb{E}_f \{[h(X)-\mathbb{E}_f (h(X))]^2\} \quad (\textit{where}\,\, \mathbb{E}_f (h(X)) \textit{can be estimated by } \overline h_n)\\
&\leftarrow \frac{1}{n}\mathbb{E}_f \{[h(X)-\overline h_n]^2\} \quad \quad (\textit{now estimate}\,\, \mathbb{E}_f [h(X)-\overline h_n]^2)\\
&\leftarrow \frac{1}{n^2} \sum_{j=1}^n \left[h(X_j)-\overline h_n\right]^2\\
&=\frac{1}{n^2} \sum_{j=1}^n \left[h(X_j)-\frac{1}{n}\sum_{i=1}^n h(X_i)\right]^2
\end{align*}
\end{mdframed}

**Example**: For $\mathcal{N}(0,1)$, want to estimate c.d.f $\phi(t)=\displaystyle\int_{-\infty}^t \frac{1}{\sqrt{2\pi}} e^{-x}\,dx$.

Say $f(x)=\frac{1}{\sqrt{2\pi}} e^{-x}$, we can rewrite $\phi(t)=\displaystyle\int_{-\infty}^t \frac{1}{\sqrt{2\pi}} e^{-x}\,dx=\int_{-\infty}^{\infty}\mathbf{1}_{(-\infty,t)}(x) \, f(x) \,dx$.

Therefore, we can generate a sample of size $n$, $\{X_i\}_{i=1}^n \overset{iid}\sim f$, then use $\overline h_n = \displaystyle\frac{1}{n}\sum_{i=1}^n \mathbf{1}_{(-\infty,t)}(X_i)$ to estimate $\phi(t)$.

**Example** (Drawback of classical M.C. integration): If $Z \sim \mathcal{N}(0,1)$, and we are asked to evaluate $P(Z>4.5)$.

$P(Z>4.5)=\mathbb{E}_f [\mathbf{1}_{(4.5,\infty)}(Z)]$

Even though we can use classical M.C. integration: generate a sample of size $n$, $\{Z_i\}_{i=1}^n \overset{iid}\sim \mathcal{N}(0,1)$, then use $\overline h_n = \displaystyle\frac{1}{n}\sum_{i=1}^n \mathbf{1}_{(4.5,\infty)}(Z_i)$ to estimate $P(Z>4.5)$. But actually, simulating $\{Z_i\}_{i=1}^n \overset{iid}\sim \mathcal{N}(0,1)$ probably only produce a hit $Z_i\in(4.5,\infty)$ once in large amount of iterations. However, using importance sampling can solve this issue.

## Importance Sampling

Given another p.d.f $g$ s.t. $supp(g)\supseteq supp(f)$.
$$
\mathbb{E}_f [h(X)]=\int h(x) f(x)\,dx=\int h(x) \frac{f(x)}{g(x)}g(x)\,dx=\mathbb{E}_g \left[h(X) \frac{f(X)}{g(X)}\right]
$$
Now, we can generate a sample of size $n$, $\{X_i\}_{i=1}^n \overset{iid}\sim g$, then use $\overline h_n = \displaystyle\frac{1}{n}\sum_{i=1}^n h(X_i) \frac{f(X_i)}{g(X_i)}$ to estimate $\mathbb{E}_f [h(X)]=\int h(x) f(x)\,dx$.

Where $g$ is called **importance function**, $\displaystyle \frac{f(X_i)}{g(X_i)}$ is called **importance weight** for $X_i$ and $\displaystyle \left( X_i , \frac{f(X_i)}{g(X_i)} \right)$ is called **importance sample**. $\displaystyle \frac{1}{n}\sum_{i=1}^n h(X_i) \frac{f(X_i)}{g(X_i)}$ is called **importance sampling estimator** for $\mathbb{E}_f [h(X)]$.

**Example** (continue the previous example): recall that $Z \sim \mathcal{N}(0,1)$, and we are asked to evaluate $P(Z>4.5)$.

Now we can take $g$ to be density funCtion of $\mathbf{exp}(1)$ (right) truncated at $4.5$:
\begin{align*}
g(y)&=P\left(exp(1)=y|exp(1)>4.5\right)=\frac{P\left(exp(1)=y , exp(1)>4.5\right)}{P\left(exp(1)>4.5\right)}= \frac{P\left(exp(1)=y\right)}{P\left(exp(1)>4.5\right)}\\
&=\frac{e^{-y}}{1-\int_0^{4.5}e^{-x}\,dx}=\frac{e^{-y}}{1-(1-e^{-4.5})}=e^{-(y-4.5)} \quad (y> 4.5)
\end{align*}

Now we can generate a sample of size $n$, $\{Y_i\}_{i=1}^n \overset{iid}\sim g$, recall $f(x)=\frac{1}{\sqrt{2\pi}} e^{-x}$, then the importance sampling estimator for $P(Z>4.5)=\mathbb{E}_f [\mathbf{1}_{(4.5,\infty)}(Z)]$ becomes 
$$
\displaystyle\frac{1}{n}\sum_{i=1}^n \mathbf{1}_{(4.5,\infty)}(Y_i) \frac{f(Y_i)}{g(Y_i)}=
\displaystyle\frac{1}{n}\sum_{i=1}^n \mathbf{1}_{(4.5,\infty)}(Y_i) \frac{e^{-\frac{Y_i^2}{2}+Y_i-4.5}}{\sqrt{2\pi}}=
\displaystyle\frac{1}{n}\sum_{i=1}^n \frac{e^{-\frac{Y_i^2}{2}+Y_i-4.5}}{\sqrt{2\pi}}  
$$

where here $\mathbf{1}_{(4.5,\infty)}(Y_i)=1$ since $\{Y_i\}_{i=1}^n \overset{iid}\sim g$.

Remark: we need $supp(g)\supseteq supp(f)$ to make sure the value of importance weight $\frac{f(X_i)}{g(X_i)}$ is meaningful. Actually here having a weaker condition $supp(g)\supseteq supp(h \times f)$ suffices since we have the fraction $\frac{h(X_i)f(X_i)}{g(X_i)}$.

**Self-normalized version of importance sampling**

Again, we want to evaluate $\mathbb{E}_f [h(X)]$. But now $f \propto \tilde f$, $g \propto \tilde g$. Say $f=c_o f_0$ where $f_0$ is the unnormalized p.d.f, $c_0$ is the normalizing constant. And $g=c_1 g_1$, where $g_0$ is the unnormalized p.d.f, $c_1$ is the normalizing constant. 

Now, suppose we know $f_0$ and $g_0$, but possibly not $c_0$ and $c_1$. And suppose we know how to generate random sample from $g$.
\begin{align*}
\mathbb{E}_f [h(X)]&=\int h(x) f(x)\,dx\\
&=\frac{\int \frac{h(x) f(x)}{g(x)}g(x)\,dx}{\int \frac{f(x)}{g(x)}g(x)\,dx}\\
&=\frac{\int \frac{f_0(x)}{g_0(x)}h(x)g(x)\,dx}{\int \frac{f_0(x)}{g_0(x)}g(x)\,dx} \quad (\textit{now }\, w(x):=\frac{f_0(x)}{g_0(x)})\\
&=\frac{\int w(x)h(x)g(x)\,dx}{\int w(x)g(x)\,dx}= \frac{\mathbb{E}_g[w(x)h(x)]}{\mathbb{E}_g[w(x)]}
\end{align*}

Then we can generate a sample of size $n$, $\{X_i\}_{i=1}^n \overset{iid}\sim g$, and use $\hat \mu = \displaystyle\frac{\frac{1}{n}\sum_{i=1}^n w(X_i)h(X_i)}{\frac{1}{n}\sum_{i=1}^n w(X_i)}$ to estimate $\mu=\mathbb{E}_f [h(X)]$. Here $\hat\mu$ is called **Self-normalized importance sampling estimator** for $\mu$.

\begin{mdframed}
The reason why $\hat\mu \rightarrow \mu$.
$$
\hat \mu = \displaystyle\frac{\frac{1}{n}\sum_{i=1}^n \frac{f_0(X_i)}{g_0(X_i)}h(X_i)}{\frac{1}{n}\sum_{i=1}^n \frac{f_0(X_i)}{g_0(X_i)}}=
\displaystyle\frac{\frac{1}{n}\sum_{i=1}^n \frac{f(X_i)}{g(X_i)}h(X_i)}{\frac{1}{n}\sum_{i=1}^n \frac{f(X_i)}{g(X_i)}}
\,\,\overset{LLN}\rightarrow \,\,\frac{\mathbb{E}_g \big[\frac{f(X)}{g(X)}h(X)\big]}{\mathbb{E}_g \big[\frac{f(X)}{g(X)}\big]}
$$
The last term equals to $\int \frac{h(x) f(x)}{g(x)}g(x)\,dx \big/\int \frac{f(x)}{g(x)}g(x)\,dx=\int h(x) f(x)\,dx=\mathbb{E}_f [h(X)]=\mu$.
\end{mdframed}